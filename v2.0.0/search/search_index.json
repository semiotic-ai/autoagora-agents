{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Developer's Guide","text":""},{"location":"#autoagora-agents","title":"AutoAgora Agents","text":""},{"location":"#developers-guide","title":"Developer's guide","text":""},{"location":"#installation-directly-from-the-source-code","title":"Installation directly from the source code","text":"<p>To install AutoAgora directly from the source code please clone the repository and install package in the virtual environment using <code>poetry</code>: <pre><code>git clone https://github.com/semiotic-ai/autoagora-agents.git\ncd autoagora\npoetry install\n</code></pre></p>"},{"location":"#running-the-autoagora-code","title":"Running the AutoAgora code","text":"<p>All scripts should be executed in the virtual environment managed by <code>poetry</code>.</p>"},{"location":"#running-the-test-suite","title":"Running the test suite","text":"<pre><code>poetry run python -m pytest\n</code></pre>"},{"location":"#running-experiments","title":"Running Experiments","text":"<p>Currently, to run an experiment, you should specify three components: <code>--name</code> (<code>-n</code>), <code>--simulation_path</code> (<code>-s</code>), and <code>--algorithm_path</code> (<code>-a</code>). The <code>--name</code> field is just the name to give to the experiment.  Once we set up a Mongo Observer to track experiments with a MongoDB, this will help you track different experiments more efficiently. The <code>--simulation_path</code> and <code>--algorithm_path</code> fields should point to a simulation configuration file and an algorithm configuration file, respectively.</p> <p>Note: We do not use default values anywhere in our code. Every value must come from the config file.</p>"},{"location":"#simulation-config","title":"Simulation Config","text":"<p>The simulation config must be a python file. Let's break down the various components of the simulation config.</p> <p>The first thing you need to do is define a function that is captured by the simulation ingredient. The name of the function itself doesn't matter, but we use <code>config</code> here for clarity.</p> <pre><code>from simulation import simulation_ingredient  # Import the simulation ingredient\n\n@simulation_ingredient.config  # Tag as the simulation config\ndef config():\n    ...\n</code></pre> <p>We use the simulation config to construct the simulation <code>Environment</code>. As such, the config should specify the inputs to the <code>Environment</code> class' initialiser.</p> <pre><code>\"\"\"\n    distributor (dict[str, Any]): The config for the query distributor.\n    entities (list[dict[str, Any]]): The configs for each group of entities.\n    nepisodes (int): How many episodes to run.\n    ntimesteps (int): How many timesteps to run each episode for.\n\"\"\"\n</code></pre> <p><code>nepisodes</code> and <code>ntimesteps</code> are self-explanatory.</p> <p><code>distributor</code> is a dictionary that specifies the configuration of which ISA to use. See the <code>distributor</code> documentation for more details.</p> <p><code>entities</code> is a list of configs for each entity type. Each entry in the <code>entities</code> list is a dictionary. The dictionary can be of kind <code>entity</code> in which case the entity has only a state, but no action, or <code>agent</code> in which case the entity has a state and an action. Check out the <code>entity</code> documentation for more details.</p>"},{"location":"#algorithm-config","title":"Algorithm Config","text":"<p>Similarly to the simulation config, the first thing you need to do is define a function that is captured by the algorithm ingredient.</p> <pre><code>from autoagora_agents import algorithm_ingredient  # Import the algorithm ingredient\n\n@algorithm_ingredient.config  # Tag as the algorithm config\ndef config():\n    ...\n</code></pre> <p>The experiment uses the algorithm config to construct the <code>Controller</code> object, which maps the simulation to the algorithms. The <code>Controller</code> also constructs the agents. It takes two inputs: <code>seed</code> and <code>agents</code>.</p> <p>The <code>seed</code> is just the random seed set for reproducibility.</p> <p>The <code>agents</code> entry is a list of dictionaries similar to <code>entities</code> from the simulation config. In fact, <code>agents</code> does much the same for the algorithm side of the code as <code>entities</code> does for the simulation side of the code. In it, each entry is a dictionary specifying the configuration of an algorithm for a particular group. Note here that the <code>\"group\"</code> field must match the <code>\"group\"</code> field of an agent type in the <code>entities</code> list. This is how the code knows how to map between the simulation and the algorithm. Other than that, the rest of the dictionary should map to the configuration of a particular algorithm. See the <code>algorithm</code> documentation for more details.</p>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing-to-autoagora-agents","title":"Contributing to AutoAgora agents","text":"<p>First off, thanks for taking the time to contribute! \u2764\ufe0f</p> <p>All types of contributions are encouraged and valued. See the Table of Contents for different ways to help and details about how this project handles them. Please make sure to read the relevant section before making your contribution. It will make it a lot easier for us maintainers and smooth out the experience for all involved. The community looks forward to your contributions. \ud83c\udf89</p> <p>And if you like the project, but just don't have time to contribute, that's fine. There are other easy ways to support the project and show your appreciation, which we would also be very happy about:</p> <ul> <li>Star the project</li> <li>Tweet about it</li> <li>Refer this project in your project's readme</li> <li>Mention the project at local meetups and tell your friends/colleagues</li> </ul>"},{"location":"contributing/#table-of-contents","title":"Table of Contents","text":"<ul> <li>I Have a Question</li> <li>I Want To Contribute</li> <li>Reporting Bugs</li> <li>Suggesting Enhancements</li> <li>Contributing PRs</li> <li>Reviewing, Approving and Merging PRs</li> <li>Styleguides</li> <li>Code style</li> <li>Static type hints</li> <li>Testing</li> <li>Commit Messages</li> </ul>"},{"location":"contributing/#i-have-a-question","title":"I Have a Question","text":"<p>If you want to ask a question, we assume that you have read the available Documentation.</p> <p>Before you ask a question, it is best to search for existing Issues that might help you. In case you have found a suitable issue and still need clarification, you can write your question in this issue. It is also advisable to search the internet for answers first.</p> <p>If you then still feel the need to ask a question and need clarification, we recommend the following:</p> <ul> <li>Open an Issue.</li> <li>Provide as much context as you can about what you're running into.</li> <li>Provide project and platform versions (nodejs, npm, etc), depending on what seems relevant.</li> </ul> <p>We will then take care of the issue as soon as possible.</p>"},{"location":"contributing/#i-want-to-contribute","title":"I Want To Contribute","text":""},{"location":"contributing/#legal-notice","title":"Legal Notice","text":"<p>When contributing to this project, you must agree to the Developer Certificate of Origin (DCO):</p> <pre><code>Developer Certificate of Origin\nVersion 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n\nEveryone is permitted to copy and distribute verbatim copies of this\nlicense document, but changing it is not allowed.\n\n\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I\n    have the right to submit it under the open source license\n    indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best\n    of my knowledge, is covered under an appropriate open source\n    license and I have the right under that license to submit that\n    work with modifications, whether created in whole or in part\n    by me, under the same open source license (unless I am\n    permitted to submit under a different license), as indicated\n    in the file; or\n\n(c) The contribution was provided directly to me by some other\n    person who certified (a), (b) or (c) and I have not modified\n    it.\n\n(d) I understand and agree that this project and the contribution\n    are public and that a record of the contribution (including all\n    personal information I submit with it, including my sign-off) is\n    maintained indefinitely and may be redistributed consistent with\n    this project or the open source license(s) involved.\n</code></pre> <p>We require a sign-off message in every commit in your pull requests to signal your agreement with the DCO.</p> <p>Note: contributing code \"generated\" by artificial intelligence tools, such as Github Copilot would be a violation of the DCO, as it is known to plagiarize snippets of code, without the possibility of assessing license or copyright compatibility with the current project, nor complying with original license attribution clauses.</p>"},{"location":"contributing/#reporting-bugs","title":"Reporting Bugs","text":""},{"location":"contributing/#before-submitting-a-bug-report","title":"Before Submitting a Bug Report","text":"<p>A good bug report shouldn't leave others needing to chase you up for more information. Therefore, we ask you to investigate carefully, collect information and describe the issue in detail in your report. Please complete the following steps in advance to help us fix any potential bug as fast as possible.</p> <ul> <li>Make sure that you are using the latest version.</li> <li>Determine if your bug is really a bug and not an error on your side e.g. using incompatible environment components/versions (Make sure that you have read the documentation. If you are looking for support, you might want to check this section).</li> <li>To see if other users have experienced (and potentially already solved) the same issue you are having, check if there is not already a bug report existing for your bug or error in the bug tracker.</li> <li>Also make sure to search the internet (including Stack Overflow) to see if users outside of the GitHub community have discussed the issue.</li> <li>Collect information about the bug:</li> <li>Stack trace (Traceback)</li> <li>OS, Platform and Version (Windows, Linux, macOS, x86, ARM)</li> <li>Version of the interpreter, compiler, SDK, runtime environment, package manager, depending on what seems relevant.</li> <li>Possibly your input and the output</li> <li>Can you reliably reproduce the issue? And can you also reproduce it with older versions?</li> </ul>"},{"location":"contributing/#how-do-i-submit-a-good-bug-report","title":"How Do I Submit a Good Bug Report?","text":"<p>You must never report security related issues, vulnerabilities or bugs including sensitive information to the issue tracker, or elsewhere in public. Instead sensitive bugs must be submitted through this form: https://forms.gle/iWhjJPiBGcLDqw2T8.</p> <p>We use GitHub issues to track bugs and errors. If you run into an issue with the project:</p> <ul> <li>Open an Issue. (Since we can't be sure at this point whether it is a bug or not, we ask you not to talk about a bug yet and not to label the issue.)</li> <li>Explain the behavior you would expect and the actual behavior.</li> <li>Please provide as much context as possible and describe the reproduction steps that someone else can follow to recreate the issue on their own. This usually includes your code. For good bug reports you should isolate the problem and create a reduced test case.</li> <li>Provide the information you collected in the previous section.</li> </ul> <p>Once it's filed:</p> <ul> <li>The project team will label the issue accordingly.</li> <li>A team member will try to reproduce the issue with your provided steps. If there are no reproduction steps or no obvious way to reproduce the issue, the team will ask you for those steps and mark the issue as <code>needs-repro</code>. Bugs with the <code>needs-repro</code> tag will not be addressed until they are reproduced.</li> <li>If the team is able to reproduce the issue, it will be marked <code>needs-fix</code>, as well as possibly other tags (such as <code>critical</code>), and the issue will be left to be implemented by someone.</li> </ul>"},{"location":"contributing/#suggesting-enhancements","title":"Suggesting Enhancements","text":"<p>This section guides you through submitting an enhancement suggestion for AutoAgora agents, including completely new features and minor improvements to existing functionality. Following these guidelines will help maintainers and the community to understand your suggestion and find related suggestions.</p>"},{"location":"contributing/#before-submitting-an-enhancement","title":"Before Submitting an Enhancement","text":"<ul> <li>Make sure that you are using the latest version.</li> <li>Read the documentation carefully and find out if the functionality is already covered, maybe by an individual configuration.</li> <li>Perform a search to see if the enhancement has already been suggested. If it has, add a comment to the existing issue instead of opening a new one.</li> <li>Find out whether your idea fits with the scope and aims of the project. It's up to you to make a strong case to convince the project's developers of the merits of this feature. Keep in mind that we want features that will be useful to the majority of our users and not just a small subset. If you're just targeting a minority of users, consider writing an add-on/plugin library.</li> </ul>"},{"location":"contributing/#how-do-i-submit-a-good-enhancement-suggestion","title":"How Do I Submit a Good Enhancement Suggestion?","text":"<p>Enhancement suggestions are tracked as GitHub issues.</p> <ul> <li>Use a clear and descriptive title for the issue to identify the suggestion.</li> <li>Provide a step-by-step description of the suggested enhancement in as many details as possible.</li> <li>Describe the current behavior and explain which behavior you expected to see instead and why. At this point you can also tell which alternatives do not work for you.</li> <li>You may want to include screenshots and animated GIFs which help you demonstrate the steps or point out the part which the suggestion is related to. You can use this tool to record GIFs on macOS and Windows, and this tool or this tool on Linux. </li> <li>Explain why this enhancement would be useful to most AutoAgora agents users. You may also want to point out the other projects that solved it better and which could serve as inspiration.</li> </ul>"},{"location":"contributing/#contributing-prs","title":"Contributing PRs","text":"<ul> <li>PRs should match the existing code style present in the file.</li> <li>PRs affecting the public API, including adding new features, must update the public documentation.</li> <li>Comments and (possibly internal) docstrings should make the code accessible.</li> <li>You should usually open an issue about a bug or possible improvement before opening a PR with a solution.</li> <li>PRs should do a single thing, so that they are easier to review.</li> <li>For example, fix one bug, or update compatibility, rather than fixing a bunch of bugs and updating compatibility and adding a new feature.</li> <li>PRs should add tests which cover the new or fixed functionality.</li> <li>PRs that move code should not also change code, so that they are easier to review.</li> <li>If only moving code, review for correctness is not required.</li> <li>If only changing code, then the diff makes it clear what lines have changed.</li> <li>PRs with large improvements to style should not also change functionality.</li> <li>This is to avoid making large diffs that are not the focus of the PR.</li> <li>While it is often helpful to fix a few typos in comments on the way past, it is different to using a regex or formatter on the whole project to fix spacing around operators.</li> <li>PRs introducing breaking changes should make this clear when opening the PR.</li> <li>You should not push commits which commented-out tests.</li> <li>If pushing a commit for which a test is broken, use the <code>@test_broken</code> macro.</li> <li>Commenting out tests while developing locally is okay, but committing a commented-out test increases the risk of it silently not being run when it should be.</li> <li>You should not squash down commits while review is still on-going.</li> <li>Squashing commits prevents the reviewer being able to see what commits are added since the last review.</li> <li>You should help review your PRs, even though you cannot approve your own PRs.</li> <li>For instance, start the review process by commenting on why certain bits of the code changed, or highlighting places where you would particularly like reviewer feedback.</li> </ul>"},{"location":"contributing/#reviewing-approving-and-merging-prs","title":"Reviewing, Approving and Merging PRs","text":"<ul> <li>PRs must have 1 approval before they are merged.</li> <li>PR authors should not approve their own PRs.</li> <li>PRs should pass CI tests before being merged.</li> <li>PRs by people without merge rights must have approval from someone who has merge rights (who will usually then merge the PR).</li> <li>PRs by people with merge rights must have approval from someone else, who may or may not have merge rights (and then may merge their own PR).</li> <li>PRs by people with merge rights should not be merged by people other than the author (just approved).</li> <li>Review comments should be phrased as questions, as it shows you are open to new ideas.</li> <li>For instance, \u201cWhy did you change this to X? Doesn\u2019t that prevent Y?\u201d rather than \u201cYou should not have changed this, it will prevent Y\u201d.   Small review suggestions, such as typo fixes, should make use of the <code>suggested change</code> feature.</li> <li>This makes it easier and more likely for all the smaller changes to be made.</li> <li>Reviewers should continue acting as a reviewer until the PR is merged.</li> </ul>"},{"location":"contributing/#styleguides","title":"Styleguides","text":""},{"location":"contributing/#code-style","title":"Code style","text":"<p>Use Black and Isort. Before committing code, run:</p> <pre><code>poetry run black\npoetry run isort\n</code></pre>"},{"location":"contributing/#static-type-hints","title":"Static type hints","text":"<p>Use static type hints for all function inputs and outputs, as per PEP 484. Check static type consistency using Pyright:</p> <pre><code>poetry run pyright\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<p>Use Pytest. It is strongly encouraged to write tests for all new code. New tests for existing code are equally welcome. Before submitting a PR, make sure that pytest passes:</p> <pre><code>poetry run pytest\n</code></pre>"},{"location":"contributing/#commit-messages","title":"Commit Messages","text":"<p>Use Conventional Commits v1.0.0. The commit types will be used to automate SemVer.</p> <p>Sign-off on all commits if you accept the Developer Certificate of Origin. Pull requests containing commits without the sign-off will be rejected.</p>"},{"location":"contributing/#attribution","title":"Attribution","text":"<p>This guide is based on the contributing-gen. Make your own!</p> <p>With additions from SciML/ColPrac.</p>"},{"location":"code/algorithm/","title":"Algorithm","text":"<p><pre><code>graph LR\n    Algorithm --&gt; PredeterminedAlgorithm\n    Algorithm --&gt; BanditAlgorithm\n    BanditAlgorithm --&gt; VPGBandit\n    BanditAlgorithm --&gt; PPOBandit\n    PPOBandit --&gt; RollingMemoryPPOBandit\n</code></pre> </p>"},{"location":"code/algorithm/#autoagora_agents.algorithm","title":"<code>autoagora_agents.algorithm</code>","text":""},{"location":"code/algorithm/#autoagora_agents.algorithm.Algorithm","title":"<code>Algorithm</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Base class for algorithms.</p> <p>Concretions must implement :meth:<code>__call__</code>.</p> <p>Attributes:</p> <ul> <li> niterations             (<code>int</code>)         \u2013 <p>Number of times the algorithm has been called.</p> </li> <li> nupdates             (<code>int</code>)         \u2013 <p>Number of times the algorithm has been updated.</p> </li> <li> group             (<code>str</code>)         \u2013 <p>The group to which the algorithm belongs.</p> </li> <li> i             (<code>int</code>)         \u2013 <p>The index of the algorithm.</p> </li> <li> name             (<code>str</code>)         \u2013 <p>The group and index of the algorithm.</p> </li> </ul> Source code in <code>autoagora_agents/algorithm.py</code> <pre><code>class Algorithm(ABC):\n\"\"\"Base class for algorithms.\n\n    Concretions must implement :meth:`__call__`.\n\n    Attributes:\n        niterations (int): Number of times the algorithm has been called.\n        nupdates (int): Number of times the algorithm has been updated.\n        group (str): The group to which the algorithm belongs.\n        i (int): The index of the algorithm.\n        name (str): The group and index of the algorithm.\n    \"\"\"\n\n    def __init__(self, *, group: str, i: int) -&gt; None:\n        self.niterations = 0\n        self.nupdates = 0\n        self.group = group\n        self.i = i\n        self.name = f\"{group}_{i}\"\n\n    def reset(self) -&gt; None:\n\"\"\"Reset the algorithm's state.\"\"\"\n        self.niterations = 0\n\n    def update(self) -&gt; None:\n\"\"\"Update the algorithm's parameters.\"\"\"\n        self.nupdates += 1\n\n    @abstractmethod\n    def __call__(\n        self,\n        *,\n        observation: np.ndarray,\n        action: np.ndarray,\n        reward: float,\n        done: bool,\n    ) -&gt; np.ndarray:\n\"\"\"Run the algorithm forward.\n\n        Keyword Arguments:\n            observation (np.ndarray): The observation seen by the agent.\n            action (np.ndarray): The previous action taken by the agent.\n            reward (float): The reward of the agent.\n            done (bool): If True, the agent is no longer in the game.\n\n        Returns:\n            np.ndarray: The next action taken by the agent.\n        \"\"\"\n        pass\n\n    @staticmethod\n    def advantage(rewards: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Compute a simple advantage estimate.\n\n        In effect, this is just standardising the samples to N(0, 1)\n\n        Arguments:\n            rewards (torch.Tensor): The reward-history using which to compute the advantage\n\n        Returns:\n            torch.Tensor: The advantage estimate\n        \"\"\"\n        std = rewards.std()\n        if torch.isnan(std) or std == 0:\n            adv = rewards\n        else:\n            adv = (rewards - rewards.mean()) / rewards.std()\n        return torch.unsqueeze(adv, dim=1)\n</code></pre>"},{"location":"code/algorithm/#autoagora_agents.algorithm.Algorithm.reset","title":"<code>reset()</code>","text":"<p>Reset the algorithm's state.</p> Source code in <code>autoagora_agents/algorithm.py</code> <pre><code>def reset(self) -&gt; None:\n\"\"\"Reset the algorithm's state.\"\"\"\n    self.niterations = 0\n</code></pre>"},{"location":"code/algorithm/#autoagora_agents.algorithm.Algorithm.update","title":"<code>update()</code>","text":"<p>Update the algorithm's parameters.</p> Source code in <code>autoagora_agents/algorithm.py</code> <pre><code>def update(self) -&gt; None:\n\"\"\"Update the algorithm's parameters.\"\"\"\n    self.nupdates += 1\n</code></pre>"},{"location":"code/algorithm/#autoagora_agents.algorithm.Algorithm.advantage","title":"<code>advantage(rewards)</code>  <code>staticmethod</code>","text":"<p>Compute a simple advantage estimate.</p> <p>In effect, this is just standardising the samples to N(0, 1)</p> <p>Parameters:</p> <ul> <li> rewards             (<code>torch.Tensor</code>)         \u2013 <p>The reward-history using which to compute the advantage</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>         \u2013 <p>torch.Tensor: The advantage estimate</p> </li> </ul> Source code in <code>autoagora_agents/algorithm.py</code> <pre><code>@staticmethod\ndef advantage(rewards: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Compute a simple advantage estimate.\n\n    In effect, this is just standardising the samples to N(0, 1)\n\n    Arguments:\n        rewards (torch.Tensor): The reward-history using which to compute the advantage\n\n    Returns:\n        torch.Tensor: The advantage estimate\n    \"\"\"\n    std = rewards.std()\n    if torch.isnan(std) or std == 0:\n        adv = rewards\n    else:\n        adv = (rewards - rewards.mean()) / rewards.std()\n    return torch.unsqueeze(adv, dim=1)\n</code></pre>"},{"location":"code/algorithm/#autoagora_agents.algorithm.PredeterminedAlgorithm","title":"<code>PredeterminedAlgorithm</code>","text":"<p>         Bases: <code>Algorithm</code></p> <p>Change to a particular value at a given timestamp.</p> <p>Attributes:</p> <ul> <li> timestamps             (<code>list[int]</code>)         \u2013 <p>The timestamps at which to change the outputted value. Must start with 0.</p> </li> <li> vals             (<code>list[np.ndarray]</code>)         \u2013 <p>The values outputted.</p> </li> </ul> Source code in <code>autoagora_agents/algorithm.py</code> <pre><code>class PredeterminedAlgorithm(Algorithm):\n\"\"\"Change to a particular value at a given timestamp.\n\n    Attributes:\n        timestamps (list[int]): The timestamps at which to change the outputted value.\n            Must start with 0.\n        vals (list[np.ndarray]): The values outputted.\n    \"\"\"\n\n    def __init__(\n        self, *, group: str, i: int, timestamps: list[int], vals: list[np.ndarray]\n    ) -&gt; None:\n        super().__init__(group=group, i=i)\n        if timestamps[0] != 0:\n            raise ValueError(\"The first timestamp must be 0.\")\n        if len(timestamps) != len(vals):\n            raise ValueError(\"The timestamps and vals lists must have the same length\")\n        self.timestamps = timestamps\n        self.vals = vals\n        self.ix = 0\n\n    def reset(self) -&gt; None:\n        super().reset()\n        self.ix = 0\n\n    def __call__(\n        self,\n        *,\n        observation: np.ndarray,\n        action: np.ndarray,\n        reward: float,\n        done: bool,\n    ) -&gt; np.ndarray:\n        if self.ix != len(self.timestamps) - 1:\n            if self.niterations &gt;= self.timestamps[self.ix + 1]:\n                self.ix += 1\n\n        self.niterations += 1\n        return self.vals[self.ix]\n</code></pre>"},{"location":"code/algorithm/#autoagora_agents.algorithm.BanditAlgorithm","title":"<code>BanditAlgorithm</code>","text":"<p>         Bases: <code>Algorithm</code></p> <p>Algorithms that have no observation other than the reward.</p> <p>Other Parameters:</p> <ul> <li> group             (<code>str</code>)         \u2013 <p>The group to which the algorithm belongs.</p> </li> <li> i             (<code>int</code>)         \u2013 <p>The id value of the object within the group.</p> </li> <li> bufferlength             (<code>int</code>)         \u2013 <p>The length of the buffer storing historical samples.</p> </li> <li> actiondistribution             (<code>dict</code>)         \u2013 <p>The config for the distribution representing the action.</p> </li> <li> optimizer             (<code>dict</code>)         \u2013 <p>The config for the optimizer.</p> </li> </ul> <p>Attributes:</p> <ul> <li> actiondist             (<code>Distribution</code>)         \u2013 <p>The distribution modelling action-selection.</p> </li> <li> buffer             (<code>deque</code>)         \u2013 <p>The buffer storing historical samples.</p> </li> <li> optimizer             (<code>optim.Optimizer</code>)         \u2013 <p>A torch optimizer.</p> </li> </ul> Source code in <code>autoagora_agents/algorithm.py</code> <pre><code>class BanditAlgorithm(Algorithm):\n\"\"\"Algorithms that have no observation other than the reward.\n\n    Keyword Arguments:\n        group (str): The group to which the algorithm belongs.\n        i (int): The id value of the object within the group.\n        bufferlength (int): The length of the buffer storing historical samples.\n        actiondistribution (dict): The config for the distribution representing the action.\n        optimizer (dict): The config for the optimizer.\n\n    Attributes:\n        actiondist (Distribution): The distribution modelling action-selection.\n        buffer (deque): The buffer storing historical samples.\n        optimizer (optim.Optimizer): A torch optimizer.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        group: str,\n        i: int,\n        bufferlength: int,\n        actiondistribution: dict,\n        optimizer: dict,\n    ) -&gt; None:\n        super().__init__(group=group, i=i)\n\n        self.actiondist = distributionfactory(**actiondistribution)\n        self.buffer = buffer.buffer(maxlength=bufferlength)\n        optimizer[\"params\"] = self.actiondist.params\n        self.opt = optimizerfactory(**optimizer)\n\n    def reset(self):\n        super().reset()\n        self.actiondist.reset()\n        self.buffer.clear()\n\n    def __call__(\n        self,\n        *,\n        observation: np.ndarray,\n        action: np.ndarray,\n        reward: float,\n        done: bool,\n    ) -&gt; np.ndarray:\n        act = np.array(self.actiondist.sample())\n        logprob = self.actiondist.logprob(torch.as_tensor(action))\n        self.buffer.append(\n            {\n                \"reward\": reward,\n                \"action\": action,\n                \"logprob\": logprob,\n            }\n        )\n        self.niterations += 1\n        return act\n\n    def logprob(self, actions: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Compute the log probability of the action given the distribution.\n\n        Arguments:\n            actions (torch.Tensor): The actions for which to compute the log probability\n\n        Returns:\n            torch.Tensor: The log probability of the actions.\n        \"\"\"\n        return self.actiondist.logprob(actions)\n</code></pre>"},{"location":"code/algorithm/#autoagora_agents.algorithm.BanditAlgorithm.logprob","title":"<code>logprob(actions)</code>","text":"<p>Compute the log probability of the action given the distribution.</p> <p>Parameters:</p> <ul> <li> actions             (<code>torch.Tensor</code>)         \u2013 <p>The actions for which to compute the log probability</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>         \u2013 <p>torch.Tensor: The log probability of the actions.</p> </li> </ul> Source code in <code>autoagora_agents/algorithm.py</code> <pre><code>def logprob(self, actions: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Compute the log probability of the action given the distribution.\n\n    Arguments:\n        actions (torch.Tensor): The actions for which to compute the log probability\n\n    Returns:\n        torch.Tensor: The log probability of the actions.\n    \"\"\"\n    return self.actiondist.logprob(actions)\n</code></pre>"},{"location":"code/algorithm/#autoagora_agents.algorithm.VPGBandit","title":"<code>VPGBandit</code>","text":"<p>         Bases: <code>BanditAlgorithm</code></p> <p>Bandit using a Vanilla Policy Gradient update.</p> <p>Other Parameters:</p> <ul> <li> group             (<code>str</code>)         \u2013 <p>The group to which the algorithm belongs.</p> </li> <li> i             (<code>int</code>)         \u2013 <p>The id value of the object within the group.</p> </li> <li> bufferlength             (<code>int</code>)         \u2013 <p>The length of the buffer storing historical samples.</p> </li> <li> actiondistribution             (<code>dict</code>)         \u2013 <p>The config for the distribution representing the action.</p> </li> <li> optimizer             (<code>dict</code>)         \u2013 <p>The config for the optimizer.</p> </li> </ul> Source code in <code>autoagora_agents/algorithm.py</code> <pre><code>class VPGBandit(BanditAlgorithm):\n\"\"\"Bandit using a Vanilla Policy Gradient update.\n\n    Keyword Arguments:\n        group (str): The group to which the algorithm belongs.\n        i (int): The id value of the object within the group.\n        bufferlength (int): The length of the buffer storing historical samples.\n        actiondistribution (dict): The config for the distribution representing the action.\n        optimizer (dict): The config for the optimizer.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        group: str,\n        i: int,\n        bufferlength: int,\n        actiondistribution: dict,\n        optimizer: dict,\n    ) -&gt; None:\n        super().__init__(\n            group=group,\n            i=i,\n            bufferlength=bufferlength,\n            actiondistribution=actiondistribution,\n            optimizer=optimizer,\n        )\n\n    def _vpgpiloss(self, *, reward: torch.Tensor, action: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Compute the VPG policy loss.\n\n        Tries to push the policy to maximise the probability of taking actions that\n        maximise the return via an advantage function, which is an lower-variance\n        Q-function.\n\n        Keyword Arguments:\n            reward (torch.Tensor): The rewards associated with taking each action.\n            action (torch.Tensor): The actions the agent took.\n\n        Returns:\n            torch.Tensor: The policy loss\n        \"\"\"\n        adv = self.advantage(reward)\n        logprob = self.logprob(action)\n\n        # Treat the different gaussians as independent. Don't mean across them.\n        loss = -torch.mean(logprob * adv, dim=0)\n        return loss\n\n    def update(self):\n        if not buffer.isfull(self.buffer):\n            return\n        super().update()\n\n        rewards = buffer.get(\"reward\", self.buffer)\n        actions = buffer.get(\"action\", self.buffer)\n\n        loss = self._vpgpiloss(reward=rewards, action=actions)\n\n        # The fudge factor has been found to empirically be the best balance between the\n        # standard deviation growing without exploding.\n        fudgefactor = -5\n        alexisterm = torch.exp(-self.actiondist.logstddev + fudgefactor)  # type: ignore\n        loss += alexisterm\n\n        # Backprop\n        self.opt.zero_grad()\n        torch.sum(loss).backward()\n        self.opt.step()\n        self.buffer.clear()\n</code></pre>"},{"location":"code/algorithm/#autoagora_agents.algorithm.PPOBandit","title":"<code>PPOBandit</code>","text":"<p>         Bases: <code>BanditAlgorithm</code></p> <p>Bandit with a PPO update.</p> <p>Other Parameters:</p> <ul> <li> group             (<code>str</code>)         \u2013 <p>The group to which the algorithm belongs.</p> </li> <li> i             (<code>int</code>)         \u2013 <p>The id value of the object within the group.</p> </li> <li> bufferlength             (<code>int</code>)         \u2013 <p>The length of the buffer storing historical samples.</p> </li> <li> actiondistribution             (<code>dict</code>)         \u2013 <p>The config for the distribution representing the action.</p> </li> <li> optimizer             (<code>dict</code>)         \u2013 <p>The config for the optimizer.</p> </li> <li> ppoiterations             (<code>int</code>)         \u2013 <p>The number of iterations to update the policy for before stopping the update step.</p> </li> <li> epsclip             (<code>float</code>)         \u2013 <p>The clip value.</p> </li> <li> entropycoeff             (<code>float</code>)         \u2013 <p>How much to weight the entropy term in the loss.</p> </li> <li> pullbackstrength             (<code>float</code>)         \u2013 <p>How strongly to apply pullback to the initial distribution.</p> </li> <li> stddevfallback             (<code>bool</code>)         \u2013 <p>Whether to do fallback for the standard deviation.</p> </li> </ul> <p>Attributes:</p> <ul> <li> ppoiterations             (<code>int</code>)         \u2013 <p>The number of iterations to update the policy for before stopping the update step.</p> </li> <li> epsclip             (<code>float</code>)         \u2013 <p>The clip value.</p> </li> <li> entropycoeff             (<code>float</code>)         \u2013 <p>How much to weight the entropy term in the loss.</p> </li> <li> pullbackstrength             (<code>float</code>)         \u2013 <p>How strongly to apply pullback to the initial distribution.</p> </li> <li> stddevfallback             (<code>bool</code>)         \u2013 <p>Whether to do fallback for the standard deviation.</p> </li> </ul> Source code in <code>autoagora_agents/algorithm.py</code> <pre><code>class PPOBandit(BanditAlgorithm):\n\"\"\"Bandit with a PPO update.\n\n    Keyword Arguments:\n        group (str): The group to which the algorithm belongs.\n        i (int): The id value of the object within the group.\n        bufferlength (int): The length of the buffer storing historical samples.\n        actiondistribution (dict): The config for the distribution representing the action.\n        optimizer (dict): The config for the optimizer.\n        ppoiterations (int): The number of iterations to update the policy for before\n            stopping the update step.\n        epsclip (float): The clip value.\n        entropycoeff (float): How much to weight the entropy term in the loss.\n        pullbackstrength (float): How strongly to apply pullback to the initial distribution.\n        stddevfallback (bool): Whether to do fallback for the standard deviation.\n\n    Attributes:\n        ppoiterations (int): The number of iterations to update the policy for before\n            stopping the update step.\n        epsclip (float): The clip value.\n        entropycoeff (float): How much to weight the entropy term in the loss.\n        pullbackstrength (float): How strongly to apply pullback to the initial distribution.\n        stddevfallback (bool): Whether to do fallback for the standard deviation.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        group: str,\n        i: int,\n        bufferlength: int,\n        actiondistribution: dict,\n        optimizer: dict,\n        ppoiterations: int,\n        epsclip: float,\n        entropycoeff: float,\n        pullbackstrength: float,\n        stddevfallback: bool,\n    ) -&gt; None:\n        super().__init__(\n            group=group,\n            i=i,\n            bufferlength=bufferlength,\n            actiondistribution=actiondistribution,\n            optimizer=optimizer,\n        )\n        self.ppoiterations = ppoiterations\n        self.epsclip = epsclip\n        self.entropycoeff = entropycoeff\n        self.pullbackstrength = pullbackstrength\n        self.stddevfallback = stddevfallback\n\n    def _ppoloss(\n        self, *, actions: torch.Tensor, logprob: torch.Tensor, adv: torch.Tensor\n    ) -&gt; torch.Tensor:\n        nlogprob = self.actiondist.logprob(actions)\n        ratio = torch.exp(nlogprob - logprob)\n\n        loss = -torch.min(\n            ratio * adv,\n            torch.clip(ratio, min=1 - self.epsclip, max=1 + self.epsclip) * adv,\n        )\n        return loss\n\n    def _entropyloss(self) -&gt; torch.Tensor:\n\"\"\"Penalise high entropies.\"\"\"\n        return -self.actiondist.entropy() * self.entropycoeff\n\n    def _update(self) -&gt; bool:\n        if not buffer.isfull(self.buffer):\n            return False\n        super().update()\n\n        rewards = buffer.get(\"reward\", self.buffer)\n        actions = buffer.get(\"action\", self.buffer)\n\n        adv = self.advantage(rewards)\n        logprob = self.logprob(actions).detach()\n\n        for _ in range(self.ppoiterations):\n            ppoloss = self._ppoloss(actions=actions, logprob=logprob, adv=adv)\n            entropyloss = self._entropyloss()\n\n            loss = torch.mean(ppoloss + entropyloss, dim=0)\n\n            # Pullback\n            loss += (\n                torch.abs(self.actiondist.unclampedmean - self.actiondist.initial_mean)\n                * self.pullbackstrength\n            )\n\n            if self.stddevfallback:\n                diff = self.actiondist.logstddev - torch.log(\n                    self.actiondist.initial_stddev\n                )\n                loss += torch.where(\n                    diff &gt; 0.0, diff * self.pullbackstrength, torch.zeros_like(diff)\n                )\n\n            self.opt.zero_grad()\n            torch.sum(loss).backward()\n            self.opt.step()\n\n        return True\n\n    def update(self):\n        ran = self._update()\n        if ran:\n            self.buffer.clear()\n</code></pre>"},{"location":"code/algorithm/#autoagora_agents.algorithm.RollingMemoryPPOBandit","title":"<code>RollingMemoryPPOBandit</code>","text":"<p>         Bases: <code>PPOBandit</code></p> <p>Bandit with a PPO update wherein the buffer is maintained in an off-policy way.</p> <p>Other Parameters:</p> <ul> <li> group             (<code>str</code>)         \u2013 <p>The group to which the algorithm belongs.</p> </li> <li> i             (<code>int</code>)         \u2013 <p>The id value of the object within the group.</p> </li> <li> bufferlength             (<code>int</code>)         \u2013 <p>The length of the buffer storing historical samples.</p> </li> <li> actiondistribution             (<code>dict</code>)         \u2013 <p>The config for the distribution representing the action.</p> </li> <li> optimizer             (<code>dict</code>)         \u2013 <p>The config for the optimizer.</p> </li> <li> ppoiterations             (<code>int</code>)         \u2013 <p>The number of iterations to update the policy for before stopping the update step.</p> </li> <li> epsclip             (<code>float</code>)         \u2013 <p>The clip value.</p> </li> <li> entropycoeff             (<code>float</code>)         \u2013 <p>How much to weight the entropy term in the loss.</p> </li> <li> pullbackstrength             (<code>float</code>)         \u2013 <p>How strongly to apply pullback to the initial distribution.</p> </li> <li> stddevfallback             (<code>bool</code>)         \u2013 <p>Whether to do fallback for the standard deviation.</p> </li> </ul> Source code in <code>autoagora_agents/algorithm.py</code> <pre><code>class RollingMemoryPPOBandit(PPOBandit):\n\"\"\"Bandit with a PPO update wherein the buffer is maintained in an off-policy way.\n\n    Keyword Arguments:\n        group (str): The group to which the algorithm belongs.\n        i (int): The id value of the object within the group.\n        bufferlength (int): The length of the buffer storing historical samples.\n        actiondistribution (dict): The config for the distribution representing the action.\n        optimizer (dict): The config for the optimizer.\n        ppoiterations (int): The number of iterations to update the policy for before\n            stopping the update step.\n        epsclip (float): The clip value.\n        entropycoeff (float): How much to weight the entropy term in the loss.\n        pullbackstrength (float): How strongly to apply pullback to the initial distribution.\n        stddevfallback (bool): Whether to do fallback for the standard deviation.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        group: str,\n        i: int,\n        bufferlength: int,\n        actiondistribution: dict,\n        optimizer: dict,\n        ppoiterations: int,\n        epsclip: float,\n        entropycoeff: float,\n        pullbackstrength: float,\n        stddevfallback: bool,\n    ) -&gt; None:\n        super().__init__(\n            group=group,\n            i=i,\n            bufferlength=bufferlength,\n            actiondistribution=actiondistribution,\n            optimizer=optimizer,\n            ppoiterations=ppoiterations,\n            epsclip=epsclip,\n            entropycoeff=entropycoeff,\n            pullbackstrength=pullbackstrength,\n            stddevfallback=stddevfallback,\n        )\n\n    def logprob(self, _):\n        return buffer.get(\"logprob\", self.buffer).unsqueeze(dim=1)\n\n    def update(self):\n        _ = self._update()\n</code></pre>"},{"location":"code/algorithm/#autoagora_agents.algorithm.algorithmgroupfactory","title":"<code>algorithmgroupfactory(*, kind, count, **kwargs)</code>","text":"<p>Instantiate new algorithms for a particular group.</p> <p>Other Parameters:</p> <ul> <li> kind             (<code>str</code>)         \u2013 <p>The type of algorithm to instantiate. \"vpgbandit\" -&gt; VPGBandit \"ppobandit\" -&gt; PPOBandit \"rmppobandit\" -&gt; RollingMemoryPPOBandit \"predetermined\" -&gt; PredeterminedAlgorithm</p> </li> <li> count             (<code>int</code>)         \u2013 <p>The number of entities in this group.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Algorithm]</code>         \u2013 <p>list[Algorithm]: A list of instantiated algorithms.</p> </li> </ul> Source code in <code>autoagora_agents/algorithm.py</code> <pre><code>def algorithmgroupfactory(*, kind: str, count: int, **kwargs) -&gt; list[Algorithm]:\n\"\"\"Instantiate new algorithms for a particular group.\n\n    Keyword Arguments:\n        kind (str): The type of algorithm to instantiate.\n            \"vpgbandit\" -&gt; VPGBandit\n            \"ppobandit\" -&gt; PPOBandit\n            \"rmppobandit\" -&gt; RollingMemoryPPOBandit\n            \"predetermined\" -&gt; PredeterminedAlgorithm\n        count (int): The number of entities in this group.\n\n    Returns:\n        list[Algorithm]: A list of instantiated algorithms.\n    \"\"\"\n    algs = {\n        \"vpgbandit\": VPGBandit,\n        \"ppobandit\": PPOBandit,\n        \"rmppobandit\": RollingMemoryPPOBandit,\n        \"predetermined\": PredeterminedAlgorithm,\n    }\n    group = [experiment.factory(kind, algs, i=i, **kwargs) for i in range(count)]\n    return group\n</code></pre>"},{"location":"code/algorithm/#autoagora_agents.algorithm.optimizerfactory","title":"<code>optimizerfactory(*, kind, **kwargs)</code>","text":"<p>Return the requested optimiser.</p> <p>Other Parameters:</p> <ul> <li> kind             (<code>str</code>)         \u2013 <p>The type of optimiser to instantiate. \"adam\" -&gt; optim.Adam \"sgd\" -&gt; optim.SGD \"rmsprop\" -&gt; optim.RMSprop</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>optim.Optimizer</code>         \u2013 <p>optim.Optimizer: The optimiser</p> </li> </ul> Source code in <code>autoagora_agents/algorithm.py</code> <pre><code>def optimizerfactory(*, kind: str, **kwargs) -&gt; optim.Optimizer:\n\"\"\"Return the requested optimiser.\n\n    Keyword Arguments:\n        kind (str): The type of optimiser to instantiate.\n            \"adam\" -&gt; optim.Adam\n            \"sgd\" -&gt; optim.SGD\n            \"rmsprop\" -&gt; optim.RMSprop\n\n    Returns:\n        optim.Optimizer: The optimiser\n    \"\"\"\n    opts = {\"adam\": optim.Adam, \"sgd\": optim.SGD, \"rmsprop\": optim.RMSprop}\n    opt = experiment.factory(kind, opts, **kwargs)\n    return opt\n</code></pre>"},{"location":"code/buffer/","title":"Buffer","text":""},{"location":"code/buffer/#autoagora_agents.buffer","title":"<code>autoagora_agents.buffer</code>","text":""},{"location":"code/buffer/#autoagora_agents.buffer.buffer","title":"<code>buffer(*, maxlength)</code>","text":"<p>Create a buffer.</p> <p>Other Parameters:</p> <ul> <li> maxlength             (<code>int</code>)         \u2013 <p>The maximum length of the buffer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>deque[dict[str, Any]]</code>         \u2013 <p>deque[dict[str, Any]]: The empty buffer.</p> </li> </ul> Source code in <code>autoagora_agents/buffer.py</code> <pre><code>def buffer(*, maxlength: int) -&gt; deque[dict[str, Any]]:\n\"\"\"Create a buffer.\n\n    Keyword Arguments:\n        maxlength (int): The maximum length of the buffer.\n\n    Returns:\n        deque[dict[str, Any]]: The empty buffer.\n    \"\"\"\n    b: deque[dict[str, Any]] = deque(maxlen=maxlength)\n    return b\n</code></pre>"},{"location":"code/buffer/#autoagora_agents.buffer.isfull","title":"<code>isfull(b)</code>","text":"<p>Return true if the buffer is full. Else false.</p> Source code in <code>autoagora_agents/buffer.py</code> <pre><code>def isfull(b: deque[dict[str, torch.Tensor]]) -&gt; bool:\n\"\"\"Return true if the buffer is full. Else false.\"\"\"\n    return len(b) == b.maxlen\n</code></pre>"},{"location":"code/buffer/#autoagora_agents.buffer.get","title":"<code>get(k, b)</code>","text":"<p>Get key from elements of the buffer.</p> <p>Parameters:</p> <ul> <li> k             (<code>str</code>)         \u2013 <p>The key.</p> </li> <li> b             (<code>deque[dict[str, Any]]</code>)         \u2013 <p>The empty buffer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>         \u2013 <p>torch.TensorList: The matching elements</p> </li> </ul> Source code in <code>autoagora_agents/buffer.py</code> <pre><code>def get(k: str, b: deque[dict[str, Any]]) -&gt; torch.Tensor:\n\"\"\"Get key from elements of the buffer.\n\n    Arguments:\n        k (str): The key.\n        b (deque[dict[str, Any]]): The empty buffer.\n\n    Returns:\n        torch.TensorList: The matching elements\n    \"\"\"\n    return torch.as_tensor([_b[k] for _b in b])\n</code></pre>"},{"location":"code/controller/","title":"Controller","text":""},{"location":"code/controller/#autoagora_agents.controller","title":"<code>autoagora_agents.controller</code>","text":""},{"location":"code/controller/#autoagora_agents.controller.Controller","title":"<code>Controller</code>","text":"<p>Holds all algorithms and routes information to each.</p> <p>Other Parameters:</p> <ul> <li> agents             (<code>list[dict[str, Any]]</code>)         \u2013 <p>A list of the configs for each agent group.</p> </li> <li> seed             (<code>int</code>)         \u2013 <p>The seed for torch.</p> </li> </ul> <p>Attributes:</p> <ul> <li> groups             (<code>dict[str, Algorithm]</code>)         \u2013 <p>A dictionary mapping agent groups to algorithms.</p> </li> </ul> Source code in <code>autoagora_agents/controller.py</code> <pre><code>class Controller:\n\"\"\"Holds all algorithms and routes information to each.\n\n    Keyword Arguments:\n        agents (list[dict[str, Any]]): A list of the configs for each agent\n            group.\n        seed (int): The seed for torch.\n\n    Attributes:\n        groups (dict[str, Algorithm]): A dictionary mapping agent groups to algorithms.\n    \"\"\"\n\n    def __init__(self, *, agents: list[dict[str, Any]], seed: int) -&gt; None:\n        self.groups = {a[\"group\"]: algorithmgroupfactory(**a) for a in agents}\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n        random.seed(seed)\n\n    def __call__(\n        self,\n        *,\n        observations: dict[str, np.ndarray],\n        actions: dict[str, np.ndarray],\n        rewards: dict[str, float],\n        dones: dict[str, bool]\n    ) -&gt; dict[str, np.ndarray]:\n\"\"\"Call each algorithm.\n\n        Keyword Arguments:\n            observations (dict[str, np.ndarray]): The observations of each agent.\n            actions (dict[str, np.ndarray]): The action of each agent.\n            rewards (dict[str, float]): The reward received by each agent.\n            dones (dict[str, bool]): Whether each agent is done.\n\n        Returns:\n            dict[str, np.ndarray]: The next actions of each agent.\n        \"\"\"\n        acts = {}\n        for alg in self.algorithmslist:\n            acts[alg.name] = alg(\n                observation=observations[alg.name],\n                action=actions[alg.name],\n                reward=rewards[alg.name],\n                done=dones[alg.name],\n            )\n        return acts\n\n    def update(self) -&gt; None:\n\"\"\"Update each algorithm.\"\"\"\n        for alg in self.algorithmslist:\n            alg.update()\n\n    @property\n    def algorithmslist(self) -&gt; list[Algorithm]:\n\"\"\"The algorithms for agents in each group.\"\"\"\n        algs = []\n        for a in self.groups.values():\n            algs.extend(a)\n        return algs\n</code></pre>"},{"location":"code/controller/#autoagora_agents.controller.Controller.algorithmslist","title":"<code>algorithmslist: list[Algorithm]</code>  <code>property</code>","text":"<p>The algorithms for agents in each group.</p>"},{"location":"code/controller/#autoagora_agents.controller.Controller.update","title":"<code>update()</code>","text":"<p>Update each algorithm.</p> Source code in <code>autoagora_agents/controller.py</code> <pre><code>def update(self) -&gt; None:\n\"\"\"Update each algorithm.\"\"\"\n    for alg in self.algorithmslist:\n        alg.update()\n</code></pre>"},{"location":"code/distribution/","title":"Distribution","text":""},{"location":"code/distribution/#autoagora_agents.distribution","title":"<code>autoagora_agents.distribution</code>","text":""},{"location":"code/distribution/#autoagora_agents.distribution.Distribution","title":"<code>Distribution</code>","text":"<p>         Bases: <code>ABC</code></p> <p>The base class for distributions.</p> Source code in <code>autoagora_agents/distribution.py</code> <pre><code>class Distribution(ABC):\n\"\"\"The base class for distributions.\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n\n    @abstractmethod\n    def reset(self) -&gt; None:\n\"\"\"Reset the distribution to its initial values.\"\"\"\n        pass\n\n    @abstractproperty\n    def initial_mean(self) -&gt; torch.Tensor:  # type: ignore\n\"\"\"torch.Tensor: Initial mean of the distribution.\"\"\"\n        pass\n\n    @abstractproperty\n    def initial_stddev(self) -&gt; torch.Tensor:  # type: ignore\n\"\"\"torch.Tensor: Initial standard deviation of the distribution.\"\"\"\n        pass\n\n    @abstractproperty\n    def logstddev(self) -&gt; torch.Tensor:  # type: ignore\n\"\"\"torch.Tensor: The log standard deviation of the distribution.\"\"\"\n        pass\n\n    @abstractproperty\n    def mean(self) -&gt; torch.Tensor:  # type: ignore\n\"\"\"torch.Tensor: Mean of the distribution.\"\"\"\n        pass\n\n    @abstractproperty\n    def unclampedmean(self) -&gt; torch.Tensor:  # type: ignore\n\"\"\"torch.Tensor: Unclamped mean of the distribution.\"\"\"\n        pass\n\n    @abstractproperty\n    def stddev(self) -&gt; torch.Tensor:  # type: ignore\n\"\"\"torch.Tensor: Standard deviation of the distribution.\"\"\"\n        pass\n\n    @abstractproperty\n    def distribution(self) -&gt; torch.distributions.Distribution:  # type: ignore\n\"\"\"torch.distributions.Distribution: The torch distribution.\"\"\"\n        pass\n\n    @abstractproperty\n    def params(self) -&gt; list[torch.Tensor]:  # type: ignore\n\"\"\"list[torch.Tensor]: The trainable parameters.\"\"\"\n        pass\n\n    @abstractmethod\n    def sample(self) -&gt; torch.Tensor:\n\"\"\"torch.Tensor: Sample the gaussian distribution.\"\"\"\n        pass\n\n    @abstractmethod\n    def logprob(self, x: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"The log probability of the PDF at x.\n\n        Arguments:\n            x (torch.Tensor): A sample.\n\n        Returns:\n            torch.Tensor: The log probability.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def entropy(self) -&gt; torch.Tensor:\n\"\"\"The entropy of the distribution.\"\"\"\n        pass\n</code></pre>"},{"location":"code/distribution/#autoagora_agents.distribution.Distribution.reset","title":"<code>reset()</code>  <code>abstractmethod</code>","text":"<p>Reset the distribution to its initial values.</p> Source code in <code>autoagora_agents/distribution.py</code> <pre><code>@abstractmethod\ndef reset(self) -&gt; None:\n\"\"\"Reset the distribution to its initial values.\"\"\"\n    pass\n</code></pre>"},{"location":"code/distribution/#autoagora_agents.distribution.Distribution.logstddev","title":"<code>logstddev()</code>","text":"<p>torch.Tensor: The log standard deviation of the distribution.</p> Source code in <code>autoagora_agents/distribution.py</code> <pre><code>@abstractproperty\ndef logstddev(self) -&gt; torch.Tensor:  # type: ignore\n\"\"\"torch.Tensor: The log standard deviation of the distribution.\"\"\"\n    pass\n</code></pre>"},{"location":"code/distribution/#autoagora_agents.distribution.Distribution.mean","title":"<code>mean()</code>","text":"<p>torch.Tensor: Mean of the distribution.</p> Source code in <code>autoagora_agents/distribution.py</code> <pre><code>@abstractproperty\ndef mean(self) -&gt; torch.Tensor:  # type: ignore\n\"\"\"torch.Tensor: Mean of the distribution.\"\"\"\n    pass\n</code></pre>"},{"location":"code/distribution/#autoagora_agents.distribution.Distribution.unclampedmean","title":"<code>unclampedmean()</code>","text":"<p>torch.Tensor: Unclamped mean of the distribution.</p> Source code in <code>autoagora_agents/distribution.py</code> <pre><code>@abstractproperty\ndef unclampedmean(self) -&gt; torch.Tensor:  # type: ignore\n\"\"\"torch.Tensor: Unclamped mean of the distribution.\"\"\"\n    pass\n</code></pre>"},{"location":"code/distribution/#autoagora_agents.distribution.Distribution.stddev","title":"<code>stddev()</code>","text":"<p>torch.Tensor: Standard deviation of the distribution.</p> Source code in <code>autoagora_agents/distribution.py</code> <pre><code>@abstractproperty\ndef stddev(self) -&gt; torch.Tensor:  # type: ignore\n\"\"\"torch.Tensor: Standard deviation of the distribution.\"\"\"\n    pass\n</code></pre>"},{"location":"code/distribution/#autoagora_agents.distribution.Distribution.distribution","title":"<code>distribution()</code>","text":"<p>torch.distributions.Distribution: The torch distribution.</p> Source code in <code>autoagora_agents/distribution.py</code> <pre><code>@abstractproperty\ndef distribution(self) -&gt; torch.distributions.Distribution:  # type: ignore\n\"\"\"torch.distributions.Distribution: The torch distribution.\"\"\"\n    pass\n</code></pre>"},{"location":"code/distribution/#autoagora_agents.distribution.Distribution.params","title":"<code>params()</code>","text":"<p>list[torch.Tensor]: The trainable parameters.</p> Source code in <code>autoagora_agents/distribution.py</code> <pre><code>@abstractproperty\ndef params(self) -&gt; list[torch.Tensor]:  # type: ignore\n\"\"\"list[torch.Tensor]: The trainable parameters.\"\"\"\n    pass\n</code></pre>"},{"location":"code/distribution/#autoagora_agents.distribution.Distribution.sample","title":"<code>sample()</code>  <code>abstractmethod</code>","text":"<p>torch.Tensor: Sample the gaussian distribution.</p> Source code in <code>autoagora_agents/distribution.py</code> <pre><code>@abstractmethod\ndef sample(self) -&gt; torch.Tensor:\n\"\"\"torch.Tensor: Sample the gaussian distribution.\"\"\"\n    pass\n</code></pre>"},{"location":"code/distribution/#autoagora_agents.distribution.Distribution.logprob","title":"<code>logprob(x)</code>  <code>abstractmethod</code>","text":"<p>The log probability of the PDF at x.</p> <p>Parameters:</p> <ul> <li> x             (<code>torch.Tensor</code>)         \u2013 <p>A sample.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>         \u2013 <p>torch.Tensor: The log probability.</p> </li> </ul> Source code in <code>autoagora_agents/distribution.py</code> <pre><code>@abstractmethod\ndef logprob(self, x: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"The log probability of the PDF at x.\n\n    Arguments:\n        x (torch.Tensor): A sample.\n\n    Returns:\n        torch.Tensor: The log probability.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"code/distribution/#autoagora_agents.distribution.Distribution.entropy","title":"<code>entropy()</code>  <code>abstractmethod</code>","text":"<p>The entropy of the distribution.</p> Source code in <code>autoagora_agents/distribution.py</code> <pre><code>@abstractmethod\ndef entropy(self) -&gt; torch.Tensor:\n\"\"\"The entropy of the distribution.\"\"\"\n    pass\n</code></pre>"},{"location":"code/distribution/#autoagora_agents.distribution.GaussianDistribution","title":"<code>GaussianDistribution</code>","text":"<p>         Bases: <code>Distribution</code></p> <p>A Gaussian distribution.</p> <p>Other Parameters:</p> <ul> <li> intial_mean             (<code>ArrayLike</code>)         \u2013 <p>The means of each gaussian distribution. For example, for multi-product, you would set one initial mean per product.</p> </li> <li> minmean             (<code>ArrayLike</code>)         \u2013 <p>The minimum value the mean can take on.</p> </li> <li> maxmean             (<code>ArrayLike</code>)         \u2013 <p>The maximum value the mean can take on.</p> </li> <li> intial_stddev             (<code>ArrayLike</code>)         \u2013 <p>The standard deviations of each gaussian distribution.</p> </li> <li> minstddev             (<code>ArrayLike</code>)         \u2013 <p>The minimum value the standard deviation can take on.</p> </li> <li> maxstddev             (<code>ArrayLike</code>)         \u2013 <p>The maximum value the standard deviation can take on.</p> </li> </ul> <p>Attributes:</p> <ul> <li> mean             (<code>torch.Tensor</code>)         \u2013 <p>The clamped mean of the distribution.</p> </li> <li> minmean             (<code>torch.Tensor</code>)         \u2013 <p>The minimum value the mean can take on.</p> </li> <li> maxmean             (<code>torch.Tensor</code>)         \u2013 <p>The maximum value the mean can take on.</p> </li> <li> stddev             (<code>torch.Tensor</code>)         \u2013 <p>The clamped standard deviation of the distribution.</p> </li> <li> minstddev             (<code>torch.Tensor</code>)         \u2013 <p>The minimum value the standard deviation can take on.</p> </li> <li> maxstddev             (<code>torch.Tensor</code>)         \u2013 <p>The maximum value the standard deviation can take on.</p> </li> </ul> Source code in <code>autoagora_agents/distribution.py</code> <pre><code>class GaussianDistribution(Distribution):\n\"\"\"A Gaussian distribution.\n\n    Keyword Arguments:\n        intial_mean (ArrayLike): The means of each gaussian distribution. For example,\n            for multi-product, you would set one initial mean per product.\n        minmean (ArrayLike): The minimum value the mean can take on.\n        maxmean (ArrayLike): The maximum value the mean can take on.\n        intial_stddev (ArrayLike): The standard deviations of each gaussian\n            distribution.\n        minstddev (ArrayLike): The minimum value the standard deviation can take on.\n        maxstddev (ArrayLike): The maximum value the standard deviation can take on.\n\n    Attributes:\n        mean (torch.Tensor): The clamped mean of the distribution.\n        minmean (torch.Tensor): The minimum value the mean can take on.\n        maxmean (torch.Tensor): The maximum value the mean can take on.\n        stddev (torch.Tensor): The clamped standard deviation of the distribution.\n        minstddev (torch.Tensor): The minimum value the standard deviation can take on.\n        maxstddev (torch.Tensor): The maximum value the standard deviation can take on.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        initial_mean: ArrayLike,\n        minmean: ArrayLike,\n        maxmean: ArrayLike,\n        initial_stddev: ArrayLike,\n        minstddev: ArrayLike,\n        maxstddev: ArrayLike,\n    ) -&gt; None:\n        super().__init__()\n\n        self._initial_mean = torch.as_tensor(initial_mean)\n        self.maxmean = torch.as_tensor(maxmean)\n        self.minmean = torch.as_tensor(minmean)\n        self._mean = nn.parameter.Parameter(self.initial_mean)\n\n        self._initial_stddev = torch.as_tensor(initial_stddev)\n        self.maxstddev = torch.as_tensor(maxstddev)\n        self.minstddev = torch.as_tensor(minstddev)\n        self._logstddev = nn.parameter.Parameter(torch.log(self.initial_stddev))\n\n    @property\n    def mean(self) -&gt; torch.Tensor:\n        return torch.clip(self._mean, min=self.minmean, max=self.maxmean)\n\n    @property\n    def stddev(self) -&gt; torch.Tensor:\n        return torch.clip(\n            torch.exp(self.logstddev), min=self.minstddev, max=self.maxstddev\n        )\n\n    @property\n    def logstddev(self) -&gt; torch.Tensor:  # type: ignore\n\"\"\"torch.Tensor: The log standard deviation of the distribution.\"\"\"\n        return self._logstddev\n\n    @property\n    def unclampedmean(self) -&gt; torch.Tensor:  # type: ignore\n\"\"\"torch.Tensor: Unclamped mean of the distribution.\"\"\"\n        return self._mean\n\n    @property\n    def initial_mean(self) -&gt; torch.Tensor:\n        return self._initial_mean\n\n    @property\n    def initial_stddev(self) -&gt; torch.Tensor:\n        return self._initial_stddev\n\n    def reset(self) -&gt; None:\n        self._mean = nn.parameter.Parameter(self.initial_mean)\n        self._logstddev = nn.parameter.Parameter(torch.log(self.initial_stddev))\n\n    @property\n    def distribution(self) -&gt; torch.distributions.Distribution:\n        return torch.distributions.Normal(loc=self.mean, scale=self.stddev)\n\n    @property\n    def params(self) -&gt; list[torch.Tensor]:\n        return [self._mean, self._logstddev]\n\n    def sample(self) -&gt; torch.Tensor:\n        return self.distribution.rsample().detach()\n\n    def logprob(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.distribution.log_prob(x)\n\n    def entropy(self) -&gt; torch.Tensor:\n        return self.distribution.entropy()\n</code></pre>"},{"location":"code/distribution/#autoagora_agents.distribution.GaussianDistribution.logstddev","title":"<code>logstddev: torch.Tensor</code>  <code>property</code>","text":"<p>torch.Tensor: The log standard deviation of the distribution.</p>"},{"location":"code/distribution/#autoagora_agents.distribution.GaussianDistribution.unclampedmean","title":"<code>unclampedmean: torch.Tensor</code>  <code>property</code>","text":"<p>torch.Tensor: Unclamped mean of the distribution.</p>"},{"location":"code/distribution/#autoagora_agents.distribution.ScaledGaussianDistribution","title":"<code>ScaledGaussianDistribution</code>","text":"<p>         Bases: <code>GaussianDistribution</code></p> <p>A Gaussian distribution wherein the gaussian is in a scaled space.</p> <p>In the scaled space, the mean is multiplied by the inverse scale factor and then put into log space. This also applies to the bounds on the mean below.</p> <p>Other Parameters:</p> <ul> <li> scalefactor             (<code>np.ndarray</code>)         \u2013 <p>The scale factor for each gaussian distribution.</p> </li> </ul> <p>Attributes:</p> <ul> <li> scalefactor             (<code>torch.Tensor</code>)         \u2013 <p>The scale factor for each gaussian distribution.</p> </li> </ul> Source code in <code>autoagora_agents/distribution.py</code> <pre><code>class ScaledGaussianDistribution(GaussianDistribution):\n\"\"\"A Gaussian distribution wherein the gaussian is in a scaled space.\n\n    In the scaled space, the mean is multiplied by the inverse scale factor and then put\n    into log space. This also applies to the bounds on the mean below.\n\n    Keyword Arguments:\n        scalefactor (np.ndarray): The scale factor for each gaussian distribution.\n\n    Attributes:\n        scalefactor (torch.Tensor): The scale factor for each gaussian distribution.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        initial_mean: ArrayLike,\n        minmean: ArrayLike,\n        maxmean: ArrayLike,\n        initial_stddev: ArrayLike,\n        minstddev: ArrayLike,\n        maxstddev: ArrayLike,\n        scalefactor: np.ndarray,\n    ) -&gt; None:\n        self.scalefactor = torch.as_tensor(scalefactor)\n\n        super().__init__(\n            initial_mean=self.inversescale(torch.as_tensor(initial_mean)),\n            minmean=self.inversescale(torch.as_tensor(minmean)),\n            maxmean=self.inversescale(torch.as_tensor(maxmean)),\n            initial_stddev=initial_stddev,\n            maxstddev=maxstddev,\n            minstddev=minstddev,\n        )\n\n    @property\n    def invscalefactor(self) -&gt; torch.Tensor:\n\"\"\"torch.Tensor: The inverse scale factor for each gaussian distribution.\"\"\"\n        return 1 / self.scalefactor\n\n    def inversescale(self, x: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Apply the inverse scaling operation to x.\"\"\"\n        return torch.log(torch.multiply(self.invscalefactor, x))\n\n    def scale(self, x: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Apply the scaling operation to x.\"\"\"\n        return torch.multiply(self.scalefactor, torch.exp(x))\n\n    def sample(self) -&gt; torch.Tensor:\n\"\"\"Sample and return values in the scaled space.\"\"\"\n        return self.scale(self.unscaledsample())\n\n    def unscaledsample(self) -&gt; torch.Tensor:\n\"\"\"Sample and return values in the unscaled space.\"\"\"\n        return self.distribution.rsample().detach()\n\n    def logprob(self, x: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"The log probability of the PDF at x.\n\n        Arguments:\n            x (torch.Tensor): A sample in the scaled space.\n\n        Returns:\n            torch.Tensor: The log probability.\n        \"\"\"\n        y = self.inversescale(x)\n        return self.distribution.log_prob(y)\n</code></pre>"},{"location":"code/distribution/#autoagora_agents.distribution.ScaledGaussianDistribution.invscalefactor","title":"<code>invscalefactor: torch.Tensor</code>  <code>property</code>","text":"<p>torch.Tensor: The inverse scale factor for each gaussian distribution.</p>"},{"location":"code/distribution/#autoagora_agents.distribution.ScaledGaussianDistribution.inversescale","title":"<code>inversescale(x)</code>","text":"<p>Apply the inverse scaling operation to x.</p> Source code in <code>autoagora_agents/distribution.py</code> <pre><code>def inversescale(self, x: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Apply the inverse scaling operation to x.\"\"\"\n    return torch.log(torch.multiply(self.invscalefactor, x))\n</code></pre>"},{"location":"code/distribution/#autoagora_agents.distribution.ScaledGaussianDistribution.scale","title":"<code>scale(x)</code>","text":"<p>Apply the scaling operation to x.</p> Source code in <code>autoagora_agents/distribution.py</code> <pre><code>def scale(self, x: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Apply the scaling operation to x.\"\"\"\n    return torch.multiply(self.scalefactor, torch.exp(x))\n</code></pre>"},{"location":"code/distribution/#autoagora_agents.distribution.ScaledGaussianDistribution.sample","title":"<code>sample()</code>","text":"<p>Sample and return values in the scaled space.</p> Source code in <code>autoagora_agents/distribution.py</code> <pre><code>def sample(self) -&gt; torch.Tensor:\n\"\"\"Sample and return values in the scaled space.\"\"\"\n    return self.scale(self.unscaledsample())\n</code></pre>"},{"location":"code/distribution/#autoagora_agents.distribution.ScaledGaussianDistribution.unscaledsample","title":"<code>unscaledsample()</code>","text":"<p>Sample and return values in the unscaled space.</p> Source code in <code>autoagora_agents/distribution.py</code> <pre><code>def unscaledsample(self) -&gt; torch.Tensor:\n\"\"\"Sample and return values in the unscaled space.\"\"\"\n    return self.distribution.rsample().detach()\n</code></pre>"},{"location":"code/distribution/#autoagora_agents.distribution.ScaledGaussianDistribution.logprob","title":"<code>logprob(x)</code>","text":"<p>The log probability of the PDF at x.</p> <p>Parameters:</p> <ul> <li> x             (<code>torch.Tensor</code>)         \u2013 <p>A sample in the scaled space.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>         \u2013 <p>torch.Tensor: The log probability.</p> </li> </ul> Source code in <code>autoagora_agents/distribution.py</code> <pre><code>def logprob(self, x: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"The log probability of the PDF at x.\n\n    Arguments:\n        x (torch.Tensor): A sample in the scaled space.\n\n    Returns:\n        torch.Tensor: The log probability.\n    \"\"\"\n    y = self.inversescale(x)\n    return self.distribution.log_prob(y)\n</code></pre>"},{"location":"code/distribution/#autoagora_agents.distribution.DegenerateDistribution","title":"<code>DegenerateDistribution</code>","text":"<p>         Bases: <code>Distribution</code></p> <p>A degenerate (deterministic) distribution.</p> <p>Other Parameters:</p> <ul> <li> initial_value             (<code>np.ndarray</code>)         \u2013 <p>The initial value of the distribution.</p> </li> <li> minvalue             (<code>np.ndarray</code>)         \u2013 <p>The minimum value of the distribution.</p> </li> <li> maxvalue             (<code>np.ndarray</code>)         \u2013 <p>The maximum value of the distribution.</p> </li> </ul> <p>Attributes:</p> <ul> <li> initial_value             (<code>torch.Tensor</code>)         \u2013 <p>The initial value of the distribution.</p> </li> <li> minvalue             (<code>torch.Tensor</code>)         \u2013 <p>The minimum value of the distribution.</p> </li> <li> maxvalue             (<code>torch.Tensor</code>)         \u2013 <p>The maximum value of the distribution.</p> </li> <li> value             (<code>torch.Tensor</code>)         \u2013 <p>The clamped value of the distribution.</p> </li> </ul> Source code in <code>autoagora_agents/distribution.py</code> <pre><code>class DegenerateDistribution(Distribution):\n\"\"\"A degenerate (deterministic) distribution.\n\n    Keyword Arguments:\n        initial_value (np.ndarray): The initial value of the distribution.\n        minvalue (np.ndarray): The minimum value of the distribution.\n        maxvalue (np.ndarray): The maximum value of the distribution.\n\n    Attributes:\n        initial_value (torch.Tensor): The initial value of the distribution.\n        minvalue (torch.Tensor): The minimum value of the distribution.\n        maxvalue (torch.Tensor): The maximum value of the distribution.\n        value (torch.Tensor): The clamped value of the distribution.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        initial_value: np.ndarray,\n        minvalue: np.ndarray,\n        maxvalue: np.ndarray,\n    ) -&gt; None:\n        super().__init__()\n        self.initial_value = torch.as_tensor(initial_value)\n        self.minvalue = torch.as_tensor(minvalue)\n        self.maxvalue = torch.as_tensor(maxvalue)\n        self._value = nn.parameter.Parameter(self.initial_value)\n\n    @property\n    def value(self) -&gt; torch.Tensor:\n        return torch.clip(self._value, min=self.minvalue, max=self.maxvalue)\n\n    @property\n    def mean(self) -&gt; torch.Tensor:\n        return self.value\n\n    @property\n    def stddev(self) -&gt; torch.Tensor:\n        return torch.zeros_like(self.value)\n\n    @property\n    def logstddev(self) -&gt; torch.Tensor:\n        return torch.log(self.stddev)\n\n    @property\n    def unclampedmean(self) -&gt; torch.Tensor:  # type: ignore\n\"\"\"torch.Tensor: Unclamped mean of the distribution.\"\"\"\n        return self._value\n\n    @property\n    def initial_mean(self) -&gt; torch.Tensor:\n        return self.initial_value\n\n    @property\n    def initial_stddev(self) -&gt; torch.Tensor:\n        return self.stddev\n\n    @property\n    def params(self) -&gt; list[torch.Tensor]:\n        return [self._value]\n\n    def reset(self) -&gt; None:\n        self._value = nn.parameter.Parameter(self.initial_value)\n\n    def sample(self) -&gt; torch.Tensor:\n        return self.value\n\n    def logprob(self, _: torch.Tensor) -&gt; torch.Tensor:\n        return torch.zeros_like(self._value)\n\n    def entropy(self) -&gt; torch.Tensor:\n        return torch.zeros_like(self._value)\n\n    @property\n    def distribution(self) -&gt; torch.distributions.Distribution:\n        return torch.distributions.Normal(\n            loc=self.value, scale=torch.zeros_like(self.value)\n        )\n</code></pre>"},{"location":"code/distribution/#autoagora_agents.distribution.DegenerateDistribution.unclampedmean","title":"<code>unclampedmean: torch.Tensor</code>  <code>property</code>","text":"<p>torch.Tensor: Unclamped mean of the distribution.</p>"},{"location":"code/distribution/#autoagora_agents.distribution.distributionfactory","title":"<code>distributionfactory(*, kind, **kwargs)</code>","text":"<p>Instantiate a new distribution.</p> <p>Other Parameters:</p> <ul> <li> kind             (<code>str</code>)         \u2013 <p>The type of distribution to instantiate. \"gaussian\" -&gt; GaussianDistribution \"scaledgaussian\" -&gt; ScaledGaussianDistribution \"degenerate\" -&gt; DegenerateDistribution</p> </li> </ul> <p>Returns:</p> <ul> <li> Distribution(            <code>Distribution</code> )        \u2013 <p>An instantiated distribution.</p> </li> </ul> Source code in <code>autoagora_agents/distribution.py</code> <pre><code>def distributionfactory(*, kind: str, **kwargs) -&gt; Distribution:\n\"\"\"Instantiate a new distribution.\n\n    Keyword Arguments:\n        kind (str): The type of distribution to instantiate.\n            \"gaussian\" -&gt; GaussianDistribution\n            \"scaledgaussian\" -&gt; ScaledGaussianDistribution\n            \"degenerate\" -&gt; DegenerateDistribution\n\n    Returns:\n        Distribution: An instantiated distribution.\n    \"\"\"\n    dists = {\n        \"gaussian\": GaussianDistribution,\n        \"scaledgaussian\": ScaledGaussianDistribution,\n        \"degenerate\": DegenerateDistribution,\n    }\n    return experiment.factory(kind, dists, **kwargs)\n</code></pre>"},{"location":"code/distributor/","title":"Distributor","text":""},{"location":"code/distributor/#simulation.distributor","title":"<code>simulation.distributor</code>","text":""},{"location":"code/distributor/#simulation.distributor.Distributor","title":"<code>Distributor</code>","text":"<p>         Bases: <code>ABC</code></p> <p>The indexer selection algorithm base class.</p> <p>Attributes:</p> <ul> <li> source             (<code>str</code>)         \u2013 <p>The group from which the query comes. E.g., \"consumer\"</p> </li> <li> to             (<code>str</code>)         \u2013 <p>The group to which the query goes. E.g., \"indexer\"</p> </li> </ul> Source code in <code>simulation/distributor.py</code> <pre><code>class Distributor(ABC):\n\"\"\"The indexer selection algorithm base class.\n\n    Attributes:\n        source (str): The group from which the query comes. E.g., \"consumer\"\n        to (str): The group to which the query goes. E.g., \"indexer\"\n    \"\"\"\n\n    def __init__(self, *, source: str, to: str) -&gt; None:\n        super().__init__()\n        self.source = source\n        self.to = to\n\n    @abstractmethod\n    def __call__(self, *, entities: dict[str, list[Entity]]) -&gt; None:\n\"\"\"Choose how to allocate traffic from `source` to `to`.\n\n        Keyword Arguments:\n            entities (dict[str, list[Entity]]): A mapping from group names to entities\n                in said group.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"code/distributor/#simulation.distributor.SoftmaxDistributor","title":"<code>SoftmaxDistributor</code>","text":"<p>         Bases: <code>Distributor</code></p> <p>Allocates traffic via a softmax function.</p> <p>However, if an indexer's price exceeds a consumer's budget, the indexer gets 0 traffic.</p> <p>Attributes:</p> <ul> <li> minprice             (<code>float</code>)         \u2013 <p>A large, negative price so as to ensure an indexer doesn't receive traffic.</p> </li> </ul> Source code in <code>simulation/distributor.py</code> <pre><code>class SoftmaxDistributor(Distributor):\n\"\"\"Allocates traffic via a softmax function.\n\n    However, if an indexer's price exceeds a consumer's budget, the indexer gets 0 traffic.\n\n    Attributes:\n        minprice (float): A large, negative price so as to ensure an indexer doesn't receive\n            traffic.\n    \"\"\"\n\n    def __init__(self, *, source: str, to: str) -&gt; None:\n        super().__init__(source=source, to=to)\n        self.minprice = -1e20\n\n    @staticmethod\n    def softmaxmask(x: np.ndarray, mask: np.ndarray) -&gt; np.ndarray:\n\"\"\"Compute the columnwise softmax for elements that are True in the mask.\n\n        Arguments:\n            x (np.ndarray): The array for which to compute the softmax.\n            mask (np.ndarray): The mask array. The function works for True values.\n\n        Returns:\n            np.ndarray: An array in which the indices that are True in the mask are\n                columnwise softmaxed, and the indices that are False are zeroed.\n        \"\"\"\n        x = np.atleast_2d(x)\n        mask = np.atleast_2d(mask)\n        y = np.zeros_like(x)\n        # Iterate over columns\n        for j in range(x.shape[1]):\n            # Get masked column\n            xmask = x[:, j][mask[:, j]]\n            if xmask.size &lt;= 0:\n                continue\n            # Subtract max for numerical stability as np.exp(inf) is inf\n            # but np.exp(-inf) is 0\n            ex = np.exp(xmask - np.max(xmask))\n            # Set value into masked indices\n            y[:, j][mask[:, j]] = ex / np.sum(ex)\n        return y\n\n    def __call__(self, *, entities: dict[str, list[Entity]]) -&gt; None:\n        source = entities[self.source]\n        to = entities[self.to]\n        prices = np.atleast_2d(np.vstack([t.state.value for t in to]))\n        traffics = np.zeros_like(prices)\n        for s in source:\n            budget = s.state.value\n            # If above budget, don't get any traffic\n            mask = prices &lt;= budget\n            # Compute how much traffic goes to each agent below the budget\n            percenttraffic = self.softmaxmask(prices, mask)\n            # Accumulate traffic values per agent\n            traffics += np.multiply(percenttraffic, s.state.traffic)\n\n        for traffic, t in zip(traffics, to):\n            t.state.traffic = traffic\n</code></pre>"},{"location":"code/distributor/#simulation.distributor.SoftmaxDistributor.softmaxmask","title":"<code>softmaxmask(x, mask)</code>  <code>staticmethod</code>","text":"<p>Compute the columnwise softmax for elements that are True in the mask.</p> <p>Parameters:</p> <ul> <li> x             (<code>np.ndarray</code>)         \u2013 <p>The array for which to compute the softmax.</p> </li> <li> mask             (<code>np.ndarray</code>)         \u2013 <p>The mask array. The function works for True values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>np.ndarray: An array in which the indices that are True in the mask are columnwise softmaxed, and the indices that are False are zeroed.</p> </li> </ul> Source code in <code>simulation/distributor.py</code> <pre><code>@staticmethod\ndef softmaxmask(x: np.ndarray, mask: np.ndarray) -&gt; np.ndarray:\n\"\"\"Compute the columnwise softmax for elements that are True in the mask.\n\n    Arguments:\n        x (np.ndarray): The array for which to compute the softmax.\n        mask (np.ndarray): The mask array. The function works for True values.\n\n    Returns:\n        np.ndarray: An array in which the indices that are True in the mask are\n            columnwise softmaxed, and the indices that are False are zeroed.\n    \"\"\"\n    x = np.atleast_2d(x)\n    mask = np.atleast_2d(mask)\n    y = np.zeros_like(x)\n    # Iterate over columns\n    for j in range(x.shape[1]):\n        # Get masked column\n        xmask = x[:, j][mask[:, j]]\n        if xmask.size &lt;= 0:\n            continue\n        # Subtract max for numerical stability as np.exp(inf) is inf\n        # but np.exp(-inf) is 0\n        ex = np.exp(xmask - np.max(xmask))\n        # Set value into masked indices\n        y[:, j][mask[:, j]] = ex / np.sum(ex)\n    return y\n</code></pre>"},{"location":"code/distributor/#simulation.distributor.distributorfactory","title":"<code>distributorfactory(*, kind, source, to, **kwargs)</code>","text":"<p>Instantiate a new Distributor.</p> <p>Other Parameters:</p> <ul> <li> kind             (<code>str</code>)         \u2013 <p>The type of Distributor to instantiate. \"softmax\" -&gt; SoftmaxDistributor</p> </li> <li> source             (<code>str</code>)         \u2013 <p>The group from which the query comes. E.g., \"consumer\"</p> </li> <li> to             (<code>str</code>)         \u2013 <p>The group to which the query goes. E.g., \"indexer\"</p> </li> </ul> <p>Returns:</p> <ul> <li> Distributor(            <code>Distributor</code> )        \u2013 <p>An instantiated Distributor.</p> </li> </ul> Source code in <code>simulation/distributor.py</code> <pre><code>def distributorfactory(*, kind: str, source: str, to: str, **kwargs) -&gt; Distributor:\n\"\"\"Instantiate a new Distributor.\n\n    Keyword Arguments:\n        kind (str): The type of Distributor to instantiate.\n            \"softmax\" -&gt; SoftmaxDistributor\n        source (str): The group from which the query comes. E.g., \"consumer\"\n        to (str): The group to which the query goes. E.g., \"indexer\"\n\n    Returns:\n        Distributor: An instantiated Distributor.\n    \"\"\"\n    distributors = {\"softmax\": SoftmaxDistributor}\n    return experiment.factory(kind, distributors, source=source, to=to, **kwargs)\n</code></pre>"},{"location":"code/dynamics/","title":"Dynamics","text":""},{"location":"code/dynamics/#simulation.dynamics","title":"<code>simulation.dynamics</code>","text":""},{"location":"code/dynamics/#simulation.dynamics.dynamics","title":"<code>dynamics(s, a)</code>","text":"<p>Update the state given the action.</p> <p>In this case, the new state is just the new action.</p> <p>Parameters:</p> <ul> <li> s             (<code>PriceState</code>)         \u2013 <p>The previous state</p> </li> <li> a             (<code>PriceAction</code>)         \u2013 <p>The current action</p> </li> </ul> Source code in <code>simulation/dynamics.py</code> <pre><code>@dispatch(BudgetState, BudgetAction)\ndef dynamics(s: BudgetState, a: BudgetAction) -&gt; None:  # type: ignore\n\"\"\"Update the state given the action.\n\n    In this case, the new state is just the new action.\n\n    Arguments:\n        s (PriceState): The previous state\n        a (PriceAction): The current action\n    \"\"\"\n    s.value = a.value\n</code></pre>"},{"location":"code/environment/","title":"Environment","text":""},{"location":"code/environment/#simulation.environment","title":"<code>simulation.environment</code>","text":""},{"location":"code/environment/#simulation.environment.Environment","title":"<code>Environment</code>","text":"<p>         Bases: <code>gymnasium.Env</code></p> <p>The AutoAgora Environment.</p> <p>Other Parameters:</p> <ul> <li> distributor             (<code>dict[str, Any]</code>)         \u2013 <p>The config for the distributor.</p> </li> <li> entities             (<code>list[dict[str, Any]]</code>)         \u2013 <p>The configs for each group of entities.</p> </li> </ul> <p>Attributes:</p> <ul> <li> groups             (<code>dict[str, list[Entity]]</code>)         \u2013 <p>A mapping from group names to the entities in that group.</p> </li> <li> nepisodes             (<code>int</code>)         \u2013 <p>How many episodes to run.</p> </li> <li> ntimesteps             (<code>int</code>)         \u2013 <p>How many timesteps to run each episode for.</p> </li> <li> t             (<code>int</code>)         \u2013 <p>The current timestep.</p> </li> <li> _rewards             (<code>dict[str, Reward]</code>)         \u2013 <p>A mapping from group names to the reward function of entities in that group.</p> </li> <li> seed             (<code>int</code>)         \u2013 <p>The random seed.</p> </li> </ul> Source code in <code>simulation/environment.py</code> <pre><code>class Environment(gymnasium.Env):\n\"\"\"The AutoAgora Environment.\n\n    Keyword Arguments:\n        distributor (dict[str, Any]): The config for the distributor.\n        entities (list[dict[str, Any]]): The configs for each group of entities.\n\n    Attributes:\n        groups (dict[str, list[Entity]]): A mapping from group names to the entities in\n            that group.\n        nepisodes (int): How many episodes to run.\n        ntimesteps (int): How many timesteps to run each episode for.\n        t (int): The current timestep.\n        _rewards (dict[str, Reward]): A mapping from group names to the reward function\n            of entities in that group.\n        _observations (dict[str, Observation]) A mapping from group names to that group's\n            observation function.\n        seed (int): The random seed.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        distributor: dict[str, Any],\n        entities: list[dict[str, Any]],\n        ntimesteps: int,\n        nepisodes: int,\n        seed: int,\n    ) -&gt; None:\n        super().__init__()\n        # Create entities\n        self.groups = {e[\"group\"]: entitygroupfactory(**e) for e in entities}\n        self.nepisodes = nepisodes\n        self.ntimesteps = ntimesteps\n        self.t = 0\n        self.seed = seed\n        self._rewards = {\n            e[\"group\"]: rewardfactory(rewards=e[\"reward\"])\n            for e in entities\n            if e[\"kind\"] == \"agent\"\n        }\n        self._observations = {\n            e[\"group\"]: observationfactory(observations=e[\"observation\"])\n            for e in entities\n            if e[\"kind\"] == \"agent\"\n        }\n        self.distributor = distributorfactory(**distributor)\n\n    def reset(\n        self,\n    ) -&gt; tuple[\n        dict[str, np.ndarray], dict[str, np.ndarray], dict[str, float], dict[str, bool]\n    ]:\n\"\"\"Reset the environment.\n\n        Returns:\n            observation (dict[str, np.ndarray]): The observations of the agents.\n                Each entry in the dictionary maps an agent to its observation.\n            reward (dict[str, float]): The rewards of the agents. Each entry in the\n                dictionary maps an agent to its reward.\n            done (dict[str, bool]): False if an agent is not done. True if it is. Each\n                entry in the dictionary maps an agent to its done state.\n        \"\"\"\n        self.t = 0\n        for group in self.groups.values():\n            for entity in group:\n                entity.reset()\n\n        # No reward for first timestep\n        reward = {a.name: 0.0 for a in self.agentslist}\n\n        return self.observation, self.action, reward, self.done\n\n    def step(\n        self, *, actions: dict[str, np.ndarray]\n    ) -&gt; tuple[\n        dict[str, np.ndarray], dict[str, np.ndarray], dict[str, float], dict[str, bool]\n    ]:\n\"\"\"Step the environment forward given a set of actions.\n\n        Keyword Arguments:\n            actions (dict[str, list[np.ndarray]]): The action of each agent.\n                The mapping is between group names and lists of actions.\n\n        Returns:\n            observation (dict[str, np.ndarray]): The observations of the agents.\n                Each entry in the dictionary maps an agent to its observation.\n            action (dict[str, np.ndarray]): The actions of the agents.\n                Each entry in the dictionary maps an agent to its action.\n            reward (dict[str, float]): The rewards of the agents. Each entry in the\n                dictionary maps an agent to its reward.\n            done (dict[str, bool]): False if an agent is not done. True if it is. Each\n                entry in the dictionary maps an agent to its done state.\n        \"\"\"\n        self.t += 1\n        # Update agent actions\n        for agent in self.agentslist:\n            agent.action.value = actions[agent.name]\n\n        # Update states\n        for agent in self.agentslist:\n            dynamics(agent.state, agent.action)  # type: ignore\n\n        self.distributor(entities=self.groups)\n\n        return self.observation, self.action, self.reward, self.done\n\n    def render(self):\n\"\"\"Rendering is not part of the simulation framework.\"\"\"\n        raise NotImplementedError(\"Rendering is handled by a separate library.\")\n\n    def close(self):\n\"\"\"Closing is not part of the simulation framework.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def agents(self) -&gt; dict[str, list[Agent]]:\n\"\"\"The agents in the environment.\"\"\"\n        return {k: v for (k, v) in self.groups.items() if type(v[0]) == Agent}  # type: ignore\n\n    @property\n    def agentslist(self) -&gt; list[Agent]:\n\"\"\"The agents in the environment as a list.\"\"\"\n        ags = []\n        for group in self.agents.values():\n            ags.extend(group)\n        return ags\n\n    @property\n    def entities(self) -&gt; dict[str, list[Entity]]:\n\"\"\"The entities in the environment.\"\"\"\n        return {k: v for (k, v) in self.groups.items() if type(v[0]) == Entity}  # type: ignore\n\n    @property\n    def observation(self) -&gt; dict[str, np.ndarray]:\n\"\"\"The observations of all agents in the environment.\"\"\"\n        d = {}\n        for (group, ags) in self.agents.items():\n            obsfn = self._observations[group]\n            for a in ags:\n                d[a.name] = obsfn(agent=a, entities=self.groups)\n        return d\n\n    @property\n    def reward(self) -&gt; dict[str, float]:\n\"\"\"The rewards of all agents in the environment.\"\"\"\n        d = {}\n        for (group, ags) in self.agents.items():\n            rewfn = self._rewards[group]\n            for a in ags:\n                d[a.name] = rewfn(agent=a, entities=self.groups)\n        return d\n\n    @property\n    def done(self) -&gt; dict[str, bool]:\n\"\"\"Whether each agent is done.\n\n        In our case, agents are only done if the episode is finished.\n        \"\"\"\n        d = {}\n        for a in self.agentslist:\n            d[a.name] = self.isfinished()\n        return d\n\n    @property\n    def action(self) -&gt; dict[str, np.ndarray]:\n\"\"\"Each agent's action.\"\"\"\n        d = {}\n        for a in self.agentslist:\n            d[a.name] = a.action.value\n        return d\n\n    def isfinished(self) -&gt; bool:\n\"\"\"True if t &gt;= ntimesteps. Else false.\"\"\"\n        return self.t &gt;= self.ntimesteps\n</code></pre>"},{"location":"code/environment/#simulation.environment.Environment.agents","title":"<code>agents: dict[str, list[Agent]]</code>  <code>property</code>","text":"<p>The agents in the environment.</p>"},{"location":"code/environment/#simulation.environment.Environment.agentslist","title":"<code>agentslist: list[Agent]</code>  <code>property</code>","text":"<p>The agents in the environment as a list.</p>"},{"location":"code/environment/#simulation.environment.Environment.entities","title":"<code>entities: dict[str, list[Entity]]</code>  <code>property</code>","text":"<p>The entities in the environment.</p>"},{"location":"code/environment/#simulation.environment.Environment.observation","title":"<code>observation: dict[str, np.ndarray]</code>  <code>property</code>","text":"<p>The observations of all agents in the environment.</p>"},{"location":"code/environment/#simulation.environment.Environment.reward","title":"<code>reward: dict[str, float]</code>  <code>property</code>","text":"<p>The rewards of all agents in the environment.</p>"},{"location":"code/environment/#simulation.environment.Environment.done","title":"<code>done: dict[str, bool]</code>  <code>property</code>","text":"<p>Whether each agent is done.</p> <p>In our case, agents are only done if the episode is finished.</p>"},{"location":"code/environment/#simulation.environment.Environment.action","title":"<code>action: dict[str, np.ndarray]</code>  <code>property</code>","text":"<p>Each agent's action.</p>"},{"location":"code/environment/#simulation.environment.Environment.reset","title":"<code>reset()</code>","text":"<p>Reset the environment.</p> <p>Returns:</p> <ul> <li> observation(            <code>dict[str, np.ndarray]</code> )        \u2013 <p>The observations of the agents. Each entry in the dictionary maps an agent to its observation.</p> </li> <li> reward(            <code>dict[str, float]</code> )        \u2013 <p>The rewards of the agents. Each entry in the dictionary maps an agent to its reward.</p> </li> <li> done(            <code>dict[str, bool]</code> )        \u2013 <p>False if an agent is not done. True if it is. Each entry in the dictionary maps an agent to its done state.</p> </li> </ul> Source code in <code>simulation/environment.py</code> <pre><code>def reset(\n    self,\n) -&gt; tuple[\n    dict[str, np.ndarray], dict[str, np.ndarray], dict[str, float], dict[str, bool]\n]:\n\"\"\"Reset the environment.\n\n    Returns:\n        observation (dict[str, np.ndarray]): The observations of the agents.\n            Each entry in the dictionary maps an agent to its observation.\n        reward (dict[str, float]): The rewards of the agents. Each entry in the\n            dictionary maps an agent to its reward.\n        done (dict[str, bool]): False if an agent is not done. True if it is. Each\n            entry in the dictionary maps an agent to its done state.\n    \"\"\"\n    self.t = 0\n    for group in self.groups.values():\n        for entity in group:\n            entity.reset()\n\n    # No reward for first timestep\n    reward = {a.name: 0.0 for a in self.agentslist}\n\n    return self.observation, self.action, reward, self.done\n</code></pre>"},{"location":"code/environment/#simulation.environment.Environment.step","title":"<code>step(*, actions)</code>","text":"<p>Step the environment forward given a set of actions.</p> <p>Other Parameters:</p> <ul> <li> actions             (<code>dict[str, list[np.ndarray]]</code>)         \u2013 <p>The action of each agent. The mapping is between group names and lists of actions.</p> </li> </ul> <p>Returns:</p> <ul> <li> observation(            <code>dict[str, np.ndarray]</code> )        \u2013 <p>The observations of the agents. Each entry in the dictionary maps an agent to its observation.</p> </li> <li> action(            <code>dict[str, np.ndarray]</code> )        \u2013 <p>The actions of the agents. Each entry in the dictionary maps an agent to its action.</p> </li> <li> reward(            <code>dict[str, float]</code> )        \u2013 <p>The rewards of the agents. Each entry in the dictionary maps an agent to its reward.</p> </li> <li> done(            <code>dict[str, bool]</code> )        \u2013 <p>False if an agent is not done. True if it is. Each entry in the dictionary maps an agent to its done state.</p> </li> </ul> Source code in <code>simulation/environment.py</code> <pre><code>def step(\n    self, *, actions: dict[str, np.ndarray]\n) -&gt; tuple[\n    dict[str, np.ndarray], dict[str, np.ndarray], dict[str, float], dict[str, bool]\n]:\n\"\"\"Step the environment forward given a set of actions.\n\n    Keyword Arguments:\n        actions (dict[str, list[np.ndarray]]): The action of each agent.\n            The mapping is between group names and lists of actions.\n\n    Returns:\n        observation (dict[str, np.ndarray]): The observations of the agents.\n            Each entry in the dictionary maps an agent to its observation.\n        action (dict[str, np.ndarray]): The actions of the agents.\n            Each entry in the dictionary maps an agent to its action.\n        reward (dict[str, float]): The rewards of the agents. Each entry in the\n            dictionary maps an agent to its reward.\n        done (dict[str, bool]): False if an agent is not done. True if it is. Each\n            entry in the dictionary maps an agent to its done state.\n    \"\"\"\n    self.t += 1\n    # Update agent actions\n    for agent in self.agentslist:\n        agent.action.value = actions[agent.name]\n\n    # Update states\n    for agent in self.agentslist:\n        dynamics(agent.state, agent.action)  # type: ignore\n\n    self.distributor(entities=self.groups)\n\n    return self.observation, self.action, self.reward, self.done\n</code></pre>"},{"location":"code/environment/#simulation.environment.Environment.render","title":"<code>render()</code>","text":"<p>Rendering is not part of the simulation framework.</p> Source code in <code>simulation/environment.py</code> <pre><code>def render(self):\n\"\"\"Rendering is not part of the simulation framework.\"\"\"\n    raise NotImplementedError(\"Rendering is handled by a separate library.\")\n</code></pre>"},{"location":"code/environment/#simulation.environment.Environment.close","title":"<code>close()</code>","text":"<p>Closing is not part of the simulation framework.</p> Source code in <code>simulation/environment.py</code> <pre><code>def close(self):\n\"\"\"Closing is not part of the simulation framework.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"code/environment/#simulation.environment.Environment.isfinished","title":"<code>isfinished()</code>","text":"<p>True if t &gt;= ntimesteps. Else false.</p> Source code in <code>simulation/environment.py</code> <pre><code>def isfinished(self) -&gt; bool:\n\"\"\"True if t &gt;= ntimesteps. Else false.\"\"\"\n    return self.t &gt;= self.ntimesteps\n</code></pre>"},{"location":"code/observation/","title":"Observation","text":""},{"location":"code/observation/#simulation.observation","title":"<code>simulation.observation</code>","text":""},{"location":"code/observation/#simulation.observation.Observation","title":"<code>Observation</code>","text":"<p>The Observation base class.</p> Source code in <code>simulation/observation.py</code> <pre><code>class Observation:\n\"\"\"The Observation base class.\"\"\"\n\n    def __init__(self) -&gt; None:\n        pass\n\n    def __call__(\n        self, *, agent: Agent, entities: dict[str, list[Entity]]\n    ) -&gt; np.ndarray:\n\"\"\"Generate the agent's observation.\n\n        Keyword Arguments:\n            agent (Agent): The agent for whom to generate the observation.\n            entities (dict[str, list[Entity]]): The entities in the environment.\n\n        Returns:\n            np.ndarray: The agent's observation.\n        \"\"\"\n        return np.array([])\n</code></pre>"},{"location":"code/observation/#simulation.observation.ObservationDecorator","title":"<code>ObservationDecorator</code>","text":"<p>         Bases: <code>Observation</code></p> <p>The base class for observation decorators.</p> <p>Attributes:</p> <ul> <li> observation             (<code>Observation</code>)         \u2013 <p>An instance of :class:<code>Observation</code></p> </li> </ul> Source code in <code>simulation/observation.py</code> <pre><code>class ObservationDecorator(Observation):\n\"\"\"The base class for observation decorators.\n\n    Attributes:\n        observation (Observation): An instance of :class:`Observation`\n    \"\"\"\n\n    def __init__(self, *, observation: Observation) -&gt; None:\n        super().__init__()\n        self._observation = observation\n\n    @property\n    def observation(self) -&gt; Observation:\n        return self._observation\n\n    def __call__(\n        self, *, agent: Agent, entities: dict[str, list[Entity]]\n    ) -&gt; np.ndarray:\n        return self._observation(agent=agent, entities=entities)\n</code></pre>"},{"location":"code/observation/#simulation.observation.BanditObservation","title":"<code>BanditObservation</code>","text":"<p>         Bases: <code>ObservationDecorator</code></p> <p>An empty observation for bandits.</p> Source code in <code>simulation/observation.py</code> <pre><code>class BanditObservation(ObservationDecorator):\n\"\"\"An empty observation for bandits.\"\"\"\n\n    def __init__(self, *, observation: Observation) -&gt; None:\n        super().__init__(observation=observation)\n\n    def __call__(\n        self, *, agent: Agent, entities: dict[str, list[Entity]]\n    ) -&gt; np.ndarray:\n        return np.append(\n            np.array([]), self._observation(agent=agent, entities=entities)\n        )\n</code></pre>"},{"location":"code/observation/#simulation.observation.observationfactory","title":"<code>observationfactory(*, observations)</code>","text":"<p>Instantiate an observation object.</p> <p>Other Parameters:</p> <ul> <li> observations             (<code>list[dict]</code>)         \u2013 <p>A list of the configs for each observation component that makes up the full observation. Each config must contain the \"kind\" keyword, wherein \"kind\" can be:     \"bandit\"</p> </li> </ul> <p>Returns:</p> <ul> <li> Observation(            <code>Observation</code> )        \u2013 <p>The observation object</p> </li> </ul> Source code in <code>simulation/observation.py</code> <pre><code>def observationfactory(*, observations: list[dict]) -&gt; Observation:\n\"\"\"Instantiate an observation object.\n\n    Keyword Arguments:\n        observations (list[dict]): A list of the configs for each observation component\n            that makes up the full observation. Each config must contain the \"kind\"\n            keyword, wherein \"kind\" can be:\n                \"bandit\"\n\n    Returns:\n        Observation: The observation object\n    \"\"\"\n    rdict = {\"bandit\": BanditObservation}\n    o = Observation()\n    for config in observations:\n        config[\"observation\"] = o\n        o = experiment.decoratorfactoryhelper(d=rdict, **config)  # type: ignore\n\n    return o\n</code></pre>"},{"location":"code/reward/","title":"Reward","text":""},{"location":"code/reward/#simulation.reward","title":"<code>simulation.reward</code>","text":""},{"location":"code/reward/#simulation.reward.Reward","title":"<code>Reward</code>","text":"<p>The Reward base class.</p> Source code in <code>simulation/reward.py</code> <pre><code>class Reward:\n\"\"\"The Reward base class.\"\"\"\n\n    def __init__(self) -&gt; None:\n        pass\n\n    def __call__(self, *, agent: Agent, entities: dict[str, list[Entity]]) -&gt; float:\n\"\"\"Compute the reward.\n\n        Keyword Arguments:\n            agent (Agent): The agent for whom to compute the reward.\n            entities (dict[str, list[Entity]]): The entities in the environment.\n\n        Returns:\n            float: The agent's reward.\n        \"\"\"\n        return 0.0\n</code></pre>"},{"location":"code/reward/#simulation.reward.RewardDecorator","title":"<code>RewardDecorator</code>","text":"<p>         Bases: <code>Reward</code></p> <p>The base class for reward decorators.</p> <p>Attributes:</p> <ul> <li> reward             (<code>Reward</code>)         \u2013 <p>An instance of :class:<code>Reward</code></p> </li> <li> multiplier             (<code>float</code>)         \u2013 <p>The reward is scaled by this value. Make this negative to make the reward into a penalty.</p> </li> </ul> Source code in <code>simulation/reward.py</code> <pre><code>class RewardDecorator(Reward):\n\"\"\"The base class for reward decorators.\n\n    Attributes:\n        reward (Reward): An instance of :class:`Reward`\n        multiplier (float): The reward is scaled by this value.\n            Make this negative to make the reward into a penalty.\n    \"\"\"\n\n    def __init__(self, *, reward: Reward, multiplier: float) -&gt; None:\n        super().__init__()\n        self._reward = reward\n        self.multiplier = multiplier\n\n    @property\n    def reward(self) -&gt; Reward:\n        return self._reward\n\n    def __call__(self, *, agent: Agent, entities: dict[str, list[Entity]]) -&gt; float:\n        return self._reward(agent=agent, entities=entities) * self.multiplier\n</code></pre>"},{"location":"code/reward/#simulation.reward.TrafficReward","title":"<code>TrafficReward</code>","text":"<p>         Bases: <code>RewardDecorator</code></p> <p>A reward based on how much traffic the agent sends/receives.</p> Source code in <code>simulation/reward.py</code> <pre><code>class TrafficReward(RewardDecorator):\n\"\"\"A reward based on how much traffic the agent sends/receives.\"\"\"\n\n    def __init__(self, *, reward: Reward, multiplier: float) -&gt; None:\n        super().__init__(reward=reward, multiplier=multiplier)\n\n    def __call__(self, *, agent: Agent, entities: dict[str, list[Entity]]) -&gt; float:\n        return agent.state.fee * self.multiplier + self._reward(  # type: ignore\n            agent=agent, entities=entities\n        )\n</code></pre>"},{"location":"code/reward/#simulation.reward.SumRegretRatio","title":"<code>SumRegretRatio</code>","text":"<p>         Bases: <code>RewardDecorator</code></p> <p>A reward based on the fees earned over the total possible fees.</p> <p>Attributes:</p> <ul> <li> fromgroup             (<code>str</code>)         \u2013 <p>The group name of the entities paying for queries. Probably \"consumer\" or something similar.</p> </li> </ul> Source code in <code>simulation/reward.py</code> <pre><code>class SumRegretRatio(RewardDecorator):\n\"\"\"A reward based on the fees earned over the total possible fees.\n\n    Attributes:\n        fromgroup (str): The group name of the entities paying for queries. Probably\n            \"consumer\" or something similar.\n    \"\"\"\n\n    def __init__(self, *, reward: Reward, multiplier: float, fromgroup: str) -&gt; None:\n        super().__init__(reward=reward, multiplier=multiplier)\n        self.fromgroup = fromgroup\n\n    def __call__(self, *, agent: Agent, entities: dict[str, list[Entity]]) -&gt; float:\n        consumers = entities[self.fromgroup]\n        # How much the agent could have earned. Each consumer's budget * the number of queries they sent\n        denom = np.sum([np.multiply(c.state.value, c.state.traffic) for c in consumers])\n        val = (agent.state.fee / denom) * self.multiplier  # type: ignore\n        return val + self._reward(agent=agent, entities=entities)\n</code></pre>"},{"location":"code/reward/#simulation.reward.rewardfactory","title":"<code>rewardfactory(*, rewards)</code>","text":"<p>Instantiate a reward object.</p> <p>Other Parameters:</p> <ul> <li> rewards             (<code>list[dict]</code>)         \u2013 <p>A list of the configs for each reward that make up the aggregate reward. Each config must contain the \"kind\" keyword, wherein \"kind\" can be:     \"traffic\"     \"sumregretratio\"</p> </li> </ul> <p>Returns:</p> <ul> <li> Reward(            <code>Reward</code> )        \u2013 <p>The reward object</p> </li> </ul> Source code in <code>simulation/reward.py</code> <pre><code>def rewardfactory(*, rewards: list[dict]) -&gt; Reward:\n\"\"\"Instantiate a reward object.\n\n    Keyword Arguments:\n        rewards (list[dict]): A list of the configs for each reward that make up the\n            aggregate reward. Each config must contain the \"kind\" keyword, wherein\n            \"kind\" can be:\n                \"traffic\"\n                \"sumregretratio\"\n\n    Returns:\n        Reward: The reward object\n    \"\"\"\n    rdict = {\"traffic\": TrafficReward, \"sumregretratio\": SumRegretRatio}\n    r = Reward()\n    for config in rewards:\n        config[\"reward\"] = r\n        r = experiment.decoratorfactoryhelper(d=rdict, **config)  # type: ignore\n\n    return r\n</code></pre>"}]}